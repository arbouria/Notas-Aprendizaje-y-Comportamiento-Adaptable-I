<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Elección Recurrente: Igualación – Notas de Aprendizaje y Comportamiento Adaptable</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Capítulo 10.html" rel="next">
<link href="./Capítulo 8.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-01c78b5cd655e4cd89133cf59d535862.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c849e05bb361e5087ea6a5234b1cf041.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": true,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 10,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Capítulo 9.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Elección Recurrente: Igualación</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./cover.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Notas de Aprendizaje y Comportamiento Adaptable</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Notas de Aprendizaje y Comportamiento Adaptable</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Principios de la Selección Natural</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolución de la Adaptabilidad del Comportamiento: El papel de las restricciones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Asignación de Crédito</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asignación de Crédito para Respuestas</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Correlación, Tiempo y Contingencia</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modelo de Aprendizaje por Refuerzo</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">El Modelo de Rescorla y Wagner</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Acción Como Elección</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 9.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Elección Recurrente: Igualación</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Capítulo 10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Comportamiento de Elección: Maximización Local</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#arturo-bouzas" id="toc-arturo-bouzas" class="nav-link active" data-scroll-target="#arturo-bouzas"><span class="header-section-number">11.1</span> Arturo Bouzas</a>
  <ul class="collapse">
  <li><a href="#elección-recurrente" id="toc-elección-recurrente" class="nav-link" data-scroll-target="#elección-recurrente"><span class="header-section-number">11.1.1</span> Elección Recurrente</a></li>
  <li><a href="#la-ley-de-igualación" id="toc-la-ley-de-igualación" class="nav-link" data-scroll-target="#la-ley-de-igualación"><span class="header-section-number">11.1.2</span> La Ley de Igualación</a></li>
  </ul></li>
  <li><a href="#desviaciones-de-igualación" id="toc-desviaciones-de-igualación" class="nav-link" data-scroll-target="#desviaciones-de-igualación"><span class="header-section-number">11.2</span> Desviaciones de Igualación</a>
  <ul class="collapse">
  <li><a href="#ley-generalizada-de-igualación" id="toc-ley-generalizada-de-igualación" class="nav-link" data-scroll-target="#ley-generalizada-de-igualación"><span class="header-section-number">11.2.1</span> Ley generalizada de Igualación</a></li>
  <li><a href="#igualación-como-un-mecanismo-adaptable" id="toc-igualación-como-un-mecanismo-adaptable" class="nav-link" data-scroll-target="#igualación-como-un-mecanismo-adaptable"><span class="header-section-number">11.2.2</span> Igualación como un Mecanismo Adaptable</a></li>
  <li><a href="#es-maximización-el-mecanismo-que-subyace-a-igualación" id="toc-es-maximización-el-mecanismo-que-subyace-a-igualación" class="nav-link" data-scroll-target="#es-maximización-el-mecanismo-que-subyace-a-igualación"><span class="header-section-number">11.2.3</span> ¿Es Maximización el Mecanismo que Subyace a Igualación?</a></li>
  <li><a href="#igualación-y-rentabilidad-de-las-respuestas" id="toc-igualación-y-rentabilidad-de-las-respuestas" class="nav-link" data-scroll-target="#igualación-y-rentabilidad-de-las-respuestas"><span class="header-section-number">11.2.4</span> Igualación y Rentabilidad de las Respuestas</a></li>
  <li><a href="#maximización-vs-rentabilidad" id="toc-maximización-vs-rentabilidad" class="nav-link" data-scroll-target="#maximización-vs-rentabilidad"><span class="header-section-number">11.2.5</span> Maximización vs Rentabilidad</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Elección Recurrente: Igualación</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="arturo-bouzas" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="arturo-bouzas"><span class="header-section-number">11.1</span> Arturo Bouzas</h2>
<p>Consideren las siguientes situaciones: a un agente se le presentan dos bolsas, y se le <strong>informa</strong> que una contiene $100,000 y la otra $100 a la vez que se le pide que opte por una de ellas; en un segundo escenario, al agente se le lleva a un restaurante que no volverá a visitar y se le pide elegir entre uno u otro platillo. En ambos ejemplos, la elección puede hacerse exclusivamente a partir del valor de las opciones en el momento de la elección, y anteriormente hemos visto que el agente selecciona la opción con mayor valor en el momento de decisión. Estas situaciones de elección son instancias de sistemas abiertos sin retroalimentación y siguiendo a Gallistel (fecha), les llamamos protocolos de <em>optar</em>. En las últimas décadas, la mayoría de la investigación psicológica con participantes humanos ha utilizado este tipo de protocolo.</p>
<p>Consideren ahora un experimento similar: en lugar de informar al agente acerca del contenido de las bolsas en un inicio, a este se le presentan las opciones como bolsas cerradas, y a través de múltiples iteraciones, el agente tiene que explorar y <strong>aprender</strong> acerca de los montos de dinero de las distintas bolsas o de la calidad de las distintas opciones ofrecidas por el restaurante. La presentación recurrente de las oportunidades de elección crea nuevos problemas de adaptación para el agente. Como vimos en las notas anteriores, el agente debe resolver el dilema de exploración - explotación y debe determinar si las consecuencias de las opciones cambian como una función de las elecciones y el paso del tiempo, o si estas son fijas e independientes de las elecciones y del tiempo. En un ejemplo no ya de una elección entre varios platillos de un restaurante sino de una elección entre acudir a uno de varios restaurantes, el agente debe visitar los distintos locales un buen número de veces antes de tomar la decisión de pasar a explotar uno. Sin embargo, aún tras haber estimado la opción con mayor valor y haber comenzado a explotar una opción elegida, el agente debe mantener cierta flexibilidad para modificar su elección, en aras de poder determinar si la calidad de los restaurantes varía aleatoriamente con el paso del tiempo (debido a factores como los cambios de cocinero, por ejemplo). En estos casos, la regla de elegir la opción que instantáneamente tiene más valor no es la que a largo plazo proporciona la mayor cantidad de refuerzos. En los experimentos con estos protocolos observamos una regla de respuesta probabilística.</p>
<section id="elección-recurrente" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="elección-recurrente"><span class="header-section-number">11.1.1</span> Elección Recurrente</h3>
<p>Fuera del laboratorio, lo común para los humanos y para muchas otras especies son situaciones en las cuales los individuos pueden elegir entre dos o más acciones o parcelas de forma repetida y continua, y en las cuales las elecciones alteran las opciones futuras de refuerzo. Estos problemas de adaptación son ejemplos de sistemas de retroalimentación cerrados y, siguiendo a Gallistel, les llamamos <em>problemas de asignación</em> de respuestas, tiempo o esfuerzo. Un estudiante a lo largo del día va asignando su tiempo a diferentes actividades: desayunar, transportarse, pasar tiempo en el salón de clases, estudiar en la biblioteca, conversar con amistades, ver a su pareja, ejercitarse. Al final del día, habrá una distribución de tiempos asignados a las diferentes actividades. Resulta importante considerar que cada una de estas actividades es en sí otro espacio de posibles acciones a las que se les puede dedicar tiempo. Por ejemplo, cuando están en el salón de clases, pueden atender lo que presenta el profesor o pueden ver las noticias en su celular, mandar un WA o fantasear. De esta forma, podemos estudiar problemas de asignación a diferentes escalas temporales, pero todas bajo el mismo esquema.</p>
<p>Un ejemplo adicional lo proporciona el forrajeo de una abeja que enfrenta dos diferentes parcelas con flores con polen. En este entorno, mientras más tiempo pasa la abeja visitando las flores de una de las parcelas, la disponibilidad del polen dentro de la misma va disminuyendo; al mismo tiempo, las flores en la otra parcela siguen llenas de polen. La abeja enfrenta dos problemas de adaptación: el primero es decidir cuánto tiempo agregado dedicarle a cada una de las dos parcelas como una función de la distribución de flores con alimento en cada una de las parcelas. Normalmente, en el contexto de asignación, la regla es distribuir el comportamiento a lo largo del tiempo de una forma que produzca la mayor ganancia posible. El segundo problema de adaptación es la decisión de cuándo salirse de una de las parcelas para visitar la otra. En estas notas, nos centraremos en el estudio del primer problema de adaptación: la distribución de respuestas y tiempos.</p>
<p>La forma más sencilla de estudiar experimentalmente protocolos de elección recurrente se desarrolló en el laboratorio de Skinner y se conoce como <em>programas de refuerzo concurrentes</em>. El protocolo consiste en presentarle a un organismo dos o más opciones de respuesta -teclas iluminadas en el caso de las palomas- que se encuentran disponibles todo el tiempo y que siguen programas individuales e independientes de refuerzo. (Figura). En estas notas, revisaremos los resultados obtenidos en el estado de equilibrio, una vez que los agentes han aprendido acerca de las consecuencias de cada opción de respuesta. La variable que se estudia es la distribución de respuestas o tiempos asignados.</p>
<p><span class="math display">\[\frac{R_1} {(R_1 +R_2)}\]</span></p>
<p><img src="media/17195442061930/17198803019080.jpg" class="img-fluid"></p>
<p>En los estudios que reportamos, el animal es expuesto a un par de programas de refuerzo dentro de sesiones diarias hasta que la distribución de respuestas a las distintas opciones disponibles se vuelve estable y deja de cambiar día con día. Esto toma entre 30 y 45 días. Esta rutina se repite para todos los pares de programas que se están estudiando. De cada par, se usan para el análisis los últimos cinco días en los que las elecciones son estables.</p>
</section>
<section id="la-ley-de-igualación" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="la-ley-de-igualación"><span class="header-section-number">11.1.2</span> La Ley de Igualación</h3>
<p>En 1961, Richard Herrnstein (ver foto)</p>
<p><img src="media/17195442061930/17198802610068.jpg" class="img-fluid"></p>
<p>reportó los resultados del primer estudio con programas concurrentes, en el que la respuesta de picar una de dos teclas era reforzada de acuerdo a un programa de intervalo variable. Recuerden que en estos programas el refuerzo se presenta tras la primera respuesta después de que haya transcurrido un tiempo aleatorio desde el último refuerzo. Un detalle muy importante que debe tenerse presente bajo este tipo de programa es que una vez que un refuerzo está disponible, la oportunidad de obtenerlo se retiene hasta que el animal responde a esa opción.</p>
<p>Herrnstein encontró que en éstos programas la tasa relativa de respuestas (su proporción) iguala la tasa relativa de reforzadores obtenidos:</p>
<p><span class="math display">\[\frac {R_1} {(R_1 + R_2)} = \frac {r_1} {(r_1 + r_2)}\]</span></p>
<p>El resultado es muy robusto y se ha reportado en un sinnúmero de especies. A esta relación entre tasas relativas de respuesta y refuerzo se le conoce como la <em>ley de igualación</em> y en la última década del siglo pasado fue la ley más citada en la literatura psicológica. En la siguiente figura pueden verse los resultados de tres palomas: cada punto representa los datos de los últimos cinco días para cada par de valores de los programas de intervalo variable. La proporción de respuestas (tasas relativas) va de cero a uno. (Figura).</p>
<p><img src="media/17195442061930/17198802219877.jpg" class="img-fluid"></p>
<p>Igualación sería un resultado trivial, si por cada refuerzo hubiese solo una respuesta, sin embargo, el patrón de igualación en los refuerzos también se puede obtener con un rango muy amplio de tasas relativas de respuesta que van más allá de la tasa específica del patrón de igualación.</p>
<p>Una forma más directa de estudiar la relación entre patrones de respuesta y patrones de refuerzo en protocolos de elección recurrente, es estudiando el <strong>tiempo</strong> asignado por los organismos a las diferentes opciones de refuerzo, en lugar de estudiar el número de respuestas discretas asignadas a las distintas opciones. Desde este enfoque, Rachlin y Baum estudiaron el comportamiento de las palomas: para ello, emplearon un espacio rectangular con un piso conectado a interruptores que permite medir el tiempo que una paloma pasa en cada lado del espacio rectangular. En cada uno de los dos extremos del espacio experimental había un comedero que asignaba comida de acuerdo a programas independientes de refuerzo de intervalo variable. En este experimento también se encontró que el tiempo relativo asignado por el organismo a un lugar iguala el refuerzo relativo obtenido en dicho lugar.</p>
<p><span class="math display">\[\frac {T_1} {(T_1 + T_2)} = \frac {r_1} {(r_1 + r_2)}\]</span></p>
</section>
</section>
<section id="desviaciones-de-igualación" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="desviaciones-de-igualación"><span class="header-section-number">11.2</span> Desviaciones de Igualación</h2>
<p>La igualación de las tasas relativas de respuesta al valor de las tasas relativas de refuerzo es un fenómeno muy robusto cuando ambas opciones de respuesta son reforzadas de acuerdo a programas de intervalo variable, sin embargo, se han encontrado desviaciones respecto al patrón de igualación cuando uno de los programas de refuerzo se cambio a otra regla, o cuando se establecen distintos tipos de reforzadores para las dos respuestas. Baum () reconoció dos tipos de desviaciones de igualación: Introducir el ejemplo de “ver Salir a alguien con una cantidad de frijoles de diferente tipo”. ¿Es el resultado de preferencias o de precios?</p>
<ol type="1">
<li><em>Sesgos</em>. Si en una visita al supermercado les ofrecen probar, sin ningún costo, frijoles negros o bayos, algunos de Uds. preferirán la prueba de los frijoles negros. Dada esa preferencia, si compran frijoles y ambos tienen el mismo precio, comprarán los frijoles negros. Sin embargo, qué frijoles deciden comprar depende de la diferencia en su precio modelada por su preferencia. Cuando los reforzadores para las dos respuestas son diferentes, por ejemplo, cuando una de las dos variedades de frijoles negros o bayos les brinda mayor satisfacción debido a su sabor particular, es posible que exista una preferencia por uno de ellos: esta preferencia tendrá un impacto sobre cada combinación de razones de refuerzo. Cuando esta razón es igual pero todavía se presentan diferencias en la tasa relativa de refuerzo, esta diferencia es un indicador del sesgo del organismo en favor de uno de los reforzadores. Los resultados se verían como los de la figura x: en ella, la tasa relativa de respuesta se aleja de 0.5, aún cuando la tasa relativa de refuerzo es igual para las dos opciones.</li>
</ol>
<p><img src="media/17195442061930/17200279114135.jpg" class="img-fluid"></p>
<ol start="2" type="1">
<li><em>Sensibilidad</em>. En una misma visita al supermercado, un mismo producto que desean comprar es ofrecido por dos marcas distintas a precios diferentes: uno cuesta $11.00 y el otro $5.50. La diferencia en precio es de 2 a 1, sin embargo, los valores numéricos son difíciles de discriminar y para algunos de Uds. esta diferencia será percibida como de 3 a 1, mientras que para otros, la diferencia se percibirá como de 1.5 a 1. Sensibilidad es una segunda desviación de igualación, que ocurre cuando los organismos no son linealmente sensibles a la diferencia entre las tasas de refuerzo. Esto puede deberse a distinciones en la importancia de las diferencias en el valor de las opciones o a la dificultad para discriminar entre ellas. La figura x muestra como se vería la relación entre tasas relativas de respuesta y de refuerzo bajo distintos valores de sensibilidad. Cuando la tasa relativa de respuesta no es muy sensible a las tasas de refuerzo para cada respuesta, observamos valores cercanos a la indiferencia (panel de la izquierda en la figura) y a este resultado se le conoce como sub igualación. Cuando la tasa relativa de respuestas sobrevalora las diferencias en las tasas de respuesta, observamos que se prefiere mayoritariamente la mejor opción (panel derecho en la figura) y a este fenómeno se le conoce como sobre-igualación.</li>
</ol>
<p>En resumen, el sesgo hace referencia a la preferencia del organismo por una de las opciones, la cual tiene un efecto multiplicativo al de la tasa de ocurrencia de los reforzadores para determinar las tasas de respuesta. El otro factor de desviación respecto a igualación es la sensibilidad del agente ante las diferencias en las tasas de ocurrencia de los refuerzos de las distintas opciones. Vamos a asumir que el sesgo y la sensibilidad varían independientemente el uno del otro.</p>
<p><img src="media/17195442061930/17200344071388.jpg" class="img-fluid"></p>
<section id="ley-generalizada-de-igualación" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="ley-generalizada-de-igualación"><span class="header-section-number">11.2.1</span> Ley generalizada de Igualación</h3>
<p>Para modelar el sesgo y la sensibilidad, Baum propuso una extensión de la ley de igualación que se conoce como la <em>Ley Generalizada de Igualación</em> y que captura las dos clases de desviaciones revisadas en la sección anterior. Primero propuso expresar la ley en términos de razones entre las distintas respuestas y los distintos refuerzos; y no de proporción entre una respuesta y el total de las respuestas o entre un refuerzo y el total de los refuerzos:</p>
<p><span class="math display">\[\frac {R_1} {R_2} = \frac {r_1} {r_2}\]</span></p>
<p>Como un segundo paso, Baum propuso que la razón de refuerzo es transformada por el agente, como una función de potencia con dos parámetros, similar a la propuesta por S. S. Stevens (AÑO) en la psicofísica sensorial.</p>
<p><span class="math display">\[ \frac{R_1}{R_2} = \alpha \left( \frac{r_1}{ r_2} \right)^\beta\]</span></p>
<p>Donde el parámetro <span class="math inline">\(\beta\)</span> representa que tan <em>sensible</em> es la razón de respuesta a los cambios en la razón de refuerzos y el parámetro <span class="math inline">\(\alpha\)</span> representa el *sesgo** en la preferencia por una alternativa sobre otra.</p>
<p>Una forma, visualmente más clara de ver la ecuación anterior, es su transformación logarítmica:</p>
<p><span class="math display">\[\log \frac{B_1}{B_2} = \beta \log\frac{r_1}{r_2} + \log \alpha\]</span></p>
<p>En la figura x podemos ver su comportamiento y en el simulador uds. pueden jugar con diferentes valores de los parámetros de sesgo y sensibilidad. Podemos ver que bajo la transformación logarítmica, la ecuación de potencia se convierte en una familia de líneas rectas en las que <span class="math inline">\(\beta\)</span> es la pendiente de la función y <span class="math inline">\(\alpha\)</span> es su intercepto. Cuando la sensibilidad es igual a uno y no hay sesgo (es decir, <span class="math inline">\(\alpha\)</span> = 0), la ecuación es la ley de igualación y la línea recta parte del origen. Los valores de <span class="math inline">\(\alpha\)</span> positivos o negativos generan líneas rectas paralelas a la recta de igualación. Los valores del parámetro de sensibilidad <span class="math inline">\(\beta\)</span> menores a uno representan sub-igualación y los valores mayores a uno representan sobre-igualación. En el primer caso, la importancia de la diferencia entre los refuerzos se empequeñece psicológicamente y en el segundo caso la misma diferencia se agranda.</p>
<p><img src="media/17195442061930/17200476681102.jpg" class="img-fluid"></p>
<p>En el marco de referencia de los modelos de elección basados en el valor de las consecuencias, la ecuación generalizada de igualación resulta ser una instancia de una regla de respuesta en la que la probabilidad de cada respuesta es una función de la diferencia entre reforzadores:</p>
<p><span class="math display">\[P(a_1)= F(\lambda(Qa_1 -Qa_2))\]</span></p>
<p>En nuestro caso, el parámetro <span class="math inline">\(\beta\)</span> es <span class="math inline">\(\lambda\)</span> y la función <span class="math inline">\(\F\)</span> es una función logística aplicada a la diferencia de los logaritmos de los dos refuerzos:</p>
<p><span class="math display">\[\log \frac{R_1}{R_2} = \beta (\log{r_1} - log{r_2}) + \log \alpha\]</span></p>
<p>La ecuación generalizada de igualación puede emplearse para distintos usos: ya sea evaluar la preferencia entre diferentes refuerzos (representada por el valor del parámetro alfa); investigar bajo qué condiciones se obtiene igualación perfecta, es decir, cuando <span class="math inline">\(\alfa = 1\)</span> y <span class="math inline">\(\beta = 1\)</span>; o finalmente, establecer la forma en la que las diferencias en refuerzo son transformadas en distintas distribuciones del comportamiento. El parámetro <span class="math inline">\(\beta\)</span> puede interpretarse en al menos dos formas, primero como una propiedad del sistema: de la misma forma que el exponente para las funciones psicofísicas varía dependiendo de la dimensión sensorial, la sensibilidad beta podría variar dependiendo del tipo de refuerzo. En segundo lugar, beta también puede interpretarse como el resultado de un conjunto de manipulaciones experimentales y restricciones perceptuales del sistema que imponen límites en la discriminabilidad de las diferencias de refuerzos. Para dar un ejemplo de estas dos interpretaciones plausibles ante un único fenómeno de discriminación: consideremos la diferencia en el refuerzo que generan dos sabores placenteros distintos, esta diferencia es mucho más fácil de distinguir que aquella los refuerzos que otorgan dos sonidos melódicos distintos. Esta diferencia en la facilidad o dificultad para discriminar entre dos estímulos podría deberse a una propiedad intrínseca de los sabores como fenómeno respecto a los sonidos (interpretación 1), aunque también podría deberse a que nuestro sistema sensorial (el sistema sensorial humano) tiene una mayor facilidad para distinguir refuerzos por vía de la modalidad gustativa que por vía de la modalidad auditiva (interpretación 2). La segunda interpretación sugiere que esta dificultad discriminativa podría no presentarse en otras especies con sistemas sensoriales distintos, por ejemplo, en los murciélagos, que poseen sistemas auditivos más agudos que nosotros.</p>
</section>
<section id="igualación-como-un-mecanismo-adaptable" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="igualación-como-un-mecanismo-adaptable"><span class="header-section-number">11.2.2</span> Igualación como un Mecanismo Adaptable</h3>
<p>Una respuesta común al principio de igualación es considerarlo como una instancia de un mecanismo de adaptación, seleccionado para maximizar el total de refuerzos disponibles. Para atender esta posibilidad, es necesario conocer la función que relaciona, por un lado, a la suma del total de los refuerzos, y por el otro, a las distribuciones relativas de respuesta. Con ello, se puede evaluar si el patrón de respuesta de igualación corresponde al máximo de la función.</p>
<p><span class="math display">\[r_{total}= f\left(\frac{R_1}{R_1 + R_2}\right)\]</span></p>
<p>Dado que en programas de IV los refuerzos no se cancelan hasta que se obtienen, al animal le conviene seguir visitando ambas opciones y de esa forma obtener todos los reforzadores posibles. Sin embargo, el número de posibles distribuciones de respuestas que garantizarían obtener todos los refuerzos es enorme. El organismo podría hacerlo simplemente alternando constantemente cada respuesta (0.5) o pasando casi todo el tiempo en una de las opciones con una ocasional visita a la opción no atendida. Lo sorprendente es que dentro de ese amplio rango de posibles tasas relativas de respuesta que producen maximización, lo que se observa empíricamente es la proporción de respuesta a cada opción que iguala la tasa de refuerzo relativa. La importancia de la igualación es que representa la solución observada, en equilibrio, a la multiplicidad de formas de maximizar el refuerzo total en programas concurrentes de intervalo variable.</p>
</section>
<section id="es-maximización-el-mecanismo-que-subyace-a-igualación" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="es-maximización-el-mecanismo-que-subyace-a-igualación"><span class="header-section-number">11.2.3</span> ¿Es Maximización el Mecanismo que Subyace a Igualación?</h3>
<p>Una pregunta muy diferente a la de si la igualación es un comportamiento adaptable es la de si, bajo condiciones de equilibrio, la maximización de la tasa de reforzamiento global es el “mecanismo” que guía el comportamiento del organismo y el cual subyace al patrón de respuestas observado en igualación.&nbsp;En otras notas veremos modelos en los que se maximizan diferentes variables, pero en estas nos concentramos en la maximización del número de refuerzos totales. Para comprender la pregunta, es necesario considerar que los fenómenos de maximización e igualación implican que los algoritmos que los organismos computan son diferentes para cada modelo. De acuerdo a esta versión de la maximización como mecanismo subyacente a la igualación, el algoritmo no distingue entre las dos respuestas disponibles y el refuerzo asociado con cada una de ellas: en lugar de ello, este solo computa y actualiza dos variables, la suma de refuerzos y la tasa relativa de respuestas. Noten que, debido a esto último, este modelo de acción no es una instancia de un modelo de elección basado en el valor de las respuestas individuales. En cambio, este modelo asume que los organismos cuentan con solo dos contadores, uno para la tasa relativa de respuestas y otro para la suma de los reforzadores obtenidos por las dos respuestas, sin distinguir entre su origen. Un reloj acumula el tiempo total <em>T</em>, durante el cual las dos respuestas se encuentran disponibles. El resultado del contador del total de refuerzos se divide entre el tiempo T. En estos casos, el organismo busca acceder al mayor número de refuerzos por unidad de tiempo. Este número de {r/T} representa la <em>ganancia</em> asociada con cada distribución posible de respuestas. Además, dentro de un proceso de ascenso de colina, como el visto en las notas x, la tasa relativa de respuesta se mueve en la dirección de una mayor tasa global de refuerzo hasta alcanzar un máximo, el cual puede ser local.</p>
</section>
<section id="igualación-y-rentabilidad-de-las-respuestas" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="igualación-y-rentabilidad-de-las-respuestas"><span class="header-section-number">11.2.4</span> Igualación y Rentabilidad de las Respuestas</h3>
<p>Otra explicación que puede dar cuenta del patrón de respuesta de igualación es que los organismos buscan igualar la rentabilidad de sus respuestas o tiempos. La rentabilidad es el número de refuerzos que se obtienen por tiempo o respuestas invertidos en una opción. Cuando ustedes deciden entre planes de ahorro bancario, la primera pregunta que hacen es cuál es la tasa de interés anual, lo que les permite saber cuánto ganarán anualmente por cada $1,000 pesos depositados en su cuenta. Igualación sugiere que esto es exactamente lo que hacen los agentes con la asignación de sus respuestas y tiempos: específicamente, igualación plantea que la regla que siguen los agentes es distribuir sus respuestas y tiempos de tal forma que, en equilibrio, las dos opciones tengan la misma rentabilidad. En concreto, bajo este modelo computacional, el organismo en esencia registra la tasa de refuerzo asociada con cada opción de respuesta disponible y luego distribuye sus respuestas proporcionalmente en cada opción para igualar las rentabilidades. Igualación para respuestas y tiempos también puede expresarse en forma de razones, esto es, en lugar de hablar de una frecuencia relativa de 6 de 8 (0.75) refuerzos para una respuesta, hablamos de una razón de 6 a 2 (3) refuerzos para esa respuesta:</p>
<p><span class="math display">\[\frac {T_1} {T_2} = \frac {r_1} {r_2}\]</span>; <span class="math display">\[\frac {R_1} {R_2} = \frac {r_1} {r_2}\]</span></p>
<p>Reacomodando términos:</p>
<p><span class="math display">\[\frac {r_1}{T_1} = \frac {r_2} {T_2}\]</span> ; <span class="math display">\[\frac {r_1}{R_1} = \frac {r_2} {R_2}\]</span></p>
<p>Esta nueva forma de expresar la ley de igualación refleja que lo que se iguala es la rentabilidad de las distintas opciones de respuesta y/o tiempo. De este modo, lo que los organismos igualan son lo que podemos llamar como <em>tasas locales de refuerzo</em>. Así, cuando un organismo sigue la ley de igualación, este experimentará tasas iguales de reforzamiento local en todas las opciones disponibles. Es decir, si el organismo recibe un reforzador por cada 30 respuestas a la Opción A, este ajustará su número de respuestas a la Opción B para igualar la tasa de reforzamiento local de un reforzador por cada 30 respuestas. De manera similar, si el organismo recibe un reforzador por cada 30 segundos dedicados a la Opción A, este ajustará sus respuestas para recibir un reforzador por cada 30 segundos dedicados a la Opción B.</p>
<p>Es importante recalcar que para igualar las tasas de refuerzo local (la tasa de refuerzo por respuesta o por unidad de tiempo) el número de respuestas emitido por el organismo a las distintas opciones puede variar sustancialmente.</p>
<p>Por ejemplo:</p>
<p>-Si la Opción A brinda 1 reforzador por cada 30 respuestas y la Opción B brinda 1 reforzador por cada 60 respuestas, igualar las tasas de refuerzo locales requeriría que el organismo respondiera dos veces más a la Opción B que a la Opción A.</p>
<p>-Si distintos programas IV concurrentes se encuentran operando (por ejemplo, IV 30 s para la Opción A e IV 60 s para la Opción B), igualar las tasas de refuerzo local implicaría asignar distintas cantidades de tiempo y respuestas a cada opción.</p>
<p>El organismo esencialmente ajusta su comportamiento para obtener un “mismo rendimiento” sobre el tiempo/energía que invierte en todas las alternativas, lo que frecuentemente resulta en una distribución bastante desigual de respuestas.</p>
<p>También, vale la pena notar que igualdad en las tasas de reforzamiento locales (rentabilidad) ocurre naturalmente a través de la distribución del comportamiento del organismo, independientemente de los diferentes programas de intervalo variable programados para cada opción.</p>
<p>Computacionalmente, considerar que el algoritmo de igualación opera dentro de programas concurrentes de dos respuestas implica que los agentes tienen cuatro contadores, dos para respuestas y dos para reforzadores, y adicionalmente, disponen de dos relojes que se echan a andar cuando cambian a una de las opciones y que se detienen cuando regresan a la opción visitada anteriormente. Estos relojes se usan para computar las tasas locales de refuerzo.</p>
<p>Aqui falta el ejemplo numerico.</p>
</section>
<section id="maximización-vs-rentabilidad" class="level3" data-number="11.2.5">
<h3 data-number="11.2.5" class="anchored" data-anchor-id="maximización-vs-rentabilidad"><span class="header-section-number">11.2.5</span> Maximización vs Rentabilidad</h3>
<p>¿Es posible distinguir entre estas dos interpretaciones de la elección en programas concurrentes? Consideren el siguiente escenario. Una estudiante es la única heredera de dos tías de edad avanzada. El monto de la herencia que le deja una de ellas depende del número de visitas que la sobrina le haga; por otra parte, la cantidad que le deja la otra tía tiene un tope máximo y solo depende de que ella la visite ocasionalmente. El escenario de las tías ilustra un programa concurrente, con una de las opciones reforzada con un programa de intervalo variable y la otra con un programa de razón variable. En este escenario, la estrategia óptima de la sobrina es asignar la mayor parte de sus visitas a la tía más demandante, la que ejemplifica un programa de razón, y visitar ocasionalmente a la tía que ejemplifica el programa de intervalo.</p>
<p>Herrnstein y Heyman (), llevaron al laboratorio el escenario recién descrito. Expusieron a las palomas a programas concurrentes razón variable - intervalo variable. En este arreglo, una de las respuestas es reforzada de acuerdo a una regla temporal (programa de intervalo variable), mientras que el refuerzo para la otra respuesta depende del número de ellas (programa de razón variable).</p>
<p>Para entender la lógica del experimento, se debe tener presente que en programas de razón variable, la rentabilidad de la respuesta es el número de respuestas necesario para obtener un refuerzo. Por ejemplo, la rentabilidad de un programa de razón variable 30 para una de las opciones es un reforzador por cada treinta respuestas invertidas (1/30). De acuerdo a la ley de igualación, en un programa concurrente RV30 - IVx, el número de respuestas por refuerzo dentro del programa de intervalo debe ser también de 30. Sin embargo, de acuerdo a una regla de maximización global, la distribución óptima sería responder mayoritariamente en la opción reforzada con el programa de razón y visitar ocasionalmente la opción reforzada de acuerdo al programa de intervalo.</p>
<p>La siguiente figura muestra los resultados obtenidos por Heyman y Herrnstein. Puede verse que la distribución de respuestas de las palomas iguala la distribución de refuerzos, con un sesgo en favor del programa de razón. El patrón de igualación obtenido en este programa favorece al algoritmo de rentabilidad por encima del de maximización. Experimentos más recientes confirman este resultado y resaltan la utilidad de separar el sesgo por una opción (en este caso la opción RV) de la sensibilidad de los organismos hacia las diferencias en refuerzo de los dos programas.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Capítulo 8.html" class="pagination-link" aria-label="Acción Como Elección">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Acción Como Elección</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Capítulo 10.html" class="pagination-link" aria-label="Comportamiento de Elección: Maximización Local">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Comportamiento de Elección: Maximización Local</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>