---
title: "Acción Como Elección"
author: "Arturo Bouzas" 
format: html
---
Todas las variantes de la ley del efecto nos dicen que las respuestas que son seguidas por un refuerzo incrementan en frecuencia. Si esta fuera la única información que nos proporciona este concepto, difícilmente le asignaríamos el estatus de una una ley y su utilidad para entender el comportamiento sería muy limitada. En un escenario aplicado, para la madre que quiere modificar la conducta de un hijo, la sola recomendación de reforzar la conducta que desea incrementar no resulta totalmente satisfactoria. La madre quisiera saber si es necesario reforzar cada instancia de la respuesta o solo algunas de ellas, quisiera saber si el refuerzo 20 tiene el mismo impacto que el refuerzo 10 y, finalmente, quisiera saber si el seguimiento de distintas reglas para la entrega del refuerzo marca una diferencia. 
 
En las notas sobre programas de refuerzo, le dimos ya una respuesta a la última pregunta: presentamos el comportamiento característico asociado con distintas reglas de entrega de refuerzo y, en particular, resaltamos que en programas de razón variable los organismos responden a una tasa más alta que en los programas de intervalo variable. En el escenario aplicado, la madre preguntaría si para conseguir que su hijo emita la respuesta deseada a cierta tasa, el valor de la razón debe ser de 5, 10, 50, 200 o 500 respuestas por refuerzo. La madre también quisiera saber qué diferencia hace para el comportamiento observado que la oportunidad de obtener un refuerzo sea de 1 en cada 5, 10, 20, 100, 200 o 500 minutos. 

Por lo tanto, lo que le hace falta a la ley del efecto con la que iniciamos esta nota es especificar la función que describe la relación entre la tasa de respuesta de un organismo y una medida del refuerzo. Para propósitos de estas notas, la medida del refuerzo será la tasa de ocurrencia del reforzador:

$$R_i = f (r_i)$$

Un primer paso para especificar la función aludida es encontrar la relación empírica entre la tasa de respuesta de los organismos y distintos valores dentro de programas de intervalo variable y de razón variable.

## Funciones de Respuesta para programas de intervalo variable 

Catania y Reynolds (1968) publicaron el primer estudio sistemático en el que se estudió la relación, “en equilibrio”, entre tasas de respuesta y tasas de refuerzo que resultan de variar los valores del programa de intervalo. Al hablar de “equilibrio”, se hace referencia a un protocolo en el que se entrena al animal con un valor de un programa IV y se ejecutan el número de sesiones necesarias para alcanzar un comportamiento estable; finalmente, los datos que se analizan son los correspondientes a los últimos días de exposición. El procedimiento se repite para cada uno de los programas IV estudiados. De esta manera, se determina la relación entre la tasa de refuerzo de distintos intervalos (valor de entrada de la función) y la tasa de respuesta de los organismos (valor de salida de la función).

imagen
  
En la siguiente figura se muestran los resultados del estudio de Catania y Reynolds. Puede verse que hay una relación de ganancias decrecientes entre la tasa de respuesta y la tasa de refuerzo que produce. La función crece rápidamente conforme incrementa la tasa de refuerzo (es decir, conforme aumenta el número de refuerzos por unidad de tiempo), hasta alcanzar un punto después del cual, incrementos subsecuentes en la tasa de refuerzo tienen un efecto cada vez menor sobre la tasa de respuesta del organismo. Esta función se encontró posteriormente en docenas de estudios (de Villiers y Herrnstein).

### Relación Entre Tasas Absolutas y Relativas de Respuesta

Para Herrnstein (1970) fue claro que cualquier función que relacione la tasa de respuesta (como valor de salida) a la tasa de refuerzo (como valor de entrada) debe satisfacer dos condiciones:
* primero, dar cuenta de la relación obtenida en los programas de intervalo variable cuando hay una sola opción de respuesta disponible para el organismo y
*  segundo, ser consistente con la ley de igualación, cuando hay dos o más opciones de respuesta para el organismo.

Recordemos que para los modelos de refuerzo, los animales no llevan un registro de la frecuencia relativa con la que deben emitir cada respuesta, en lugar de ello, estos modelos postulan que para el organismo cada respuesta adquiere un valor por separado, los cuales subyacen a los patrones de respuesta observados. El valor de cada respuesta varía en función del refuerzo que ésta produce. Por otro lado, la tasa relativa con la que el organismo emite cada respuesta es el resultado de una comparación entre el valor adquirido por cada una de las distintas respuestas. El objetivo, por lo tanto, es encontrar la función que describe la relación entre los tres valores: el valor adquirido por cada respuesta; la tasa con la que el organismo emite cada respuesta; y las tasas de refuerzo que resultan de cada una de las tasas de respuesta:

$$R_i = f (r_i)$$

Y usando la función *f*, derivar la relación entre las tasas relativas de respuesta y las tasas relativas de refuerzo. La función *f* la vamos a evaluar por su éxito para dar cuenta de las tasas relativas de respuesta (siguiendo el patrón de la ley de igualación) observadas en programas concurrentes IV - IV. Para ello, debemos tener presente que la programación de los refuerzos en lo programas concurrentes puede hacerse de dos formas:


* La primera es manteniendo constante la tasa de refuerzo a lo largo del experimento, pero variando la proporción asignada a cada respuesta en diferentes condiciones experimentales. Por ejemplo, se pueden programar 60 refuerzos por hora, al mismo tiempo que se establecen diferentes condiciones experimentales en las que se generan las siguientes distribuciones de refuerzos para las dos respuestas posibles: 50-10, 40-20, 30-30, 20-40 y 10-50.
* La segunda es programando una tasa de refuerzo fija para una de las respuestas a lo largo del experimento, a la vez que se varía la tasa de refuerzo para la segunda respuesta en diferentes condiciones experimentales.

La siguiente figura muestra los resultados de un experimento que compara los resultados de las dos formas de estructurar los programas concurrentes:

imagen

La figura del panel izquierdo muestra la tasa de respuesta de las dos opciones bajo la condición donde la tasa de refuerzo total es constante. La figura del panel derecho muestra la tasa de respuesta de las dos opciones bajo la condición en la cual la tasa de refuerzo para una respuesta (símbolos negros) es constante, mientras que la tasa de refuerzo para la otra respuesta es variable (círculos abiertos). Podemos ver que en la primera condición, la tasa de respuesta está linealmente relacionada a su tasa de refuerzo. A mayor tasa de refuerzo, mayor tasa de respuesta. Mientras tanto, en la segunda condición, la tasa de respuesta a la opción con refuerzo fijo disminuye en función del aumento en el refuerzo obtenido en la otra opción de respuesta.

En otras palabras, la diferencia sútil pero crucial entre las dos condiciones reside en cómo la tasa de respuesta de una opción varía con relación a la tasa de respuesta de la otra opción dentro de una misma condición.

En la primera condición (refuerzo total constante), si la tasa de refuerzo de una opción aumenta, la tasa de refuerzo de la otra opción decrementa proporcionalmente (dado que el total se encuentra fijo). Esto genera una compensación perfectamente lineal a nivel de las tasas de refuerzo: y las tasas de respuesta reflejan esta relación lineal porque esencialmente ambas opciones compiten por un pool de respuestas fijo. 

En la segunda condición (una opción con refuerzo fijo, la otra con refuerzo variable), el aumento a la tasa de refuerzo para la opción variable disminuye la tasa de respuesta para la opción fija. A pesar de que la tasa de refuerzo de la opción fija no ha cambiado en absoluto, las respuestas a ella disminuyen de todas formas cuando la segunda opción se vuelve más recompensante. Esto demuestra que el organismo no mantiene meramente un registro de las tasas de refuerzo absolutas, sino que ejecuta una computación más compleja sobre el valor relativo de cada opción dentro del contexto más amplio. 

### Posibles Funciones de Refuerzo

La primera función que podemos considerar tiene su origen en una propuesta de Skinner. De acuerdo a esta, la tasa de respuesta es directamente proporcional  a la tasa de refuerzo.

$$R_i = kr_i$$

La ecuación describe una línea recta, donde el parámetro **k** es su pendiente y representa la traducción de un refuerzo en un incremento fijo en la tasa de respuesta. (Puede trabajar con el correspondiente simulador).

Para determinar si esta primera función es consistente con la ley de igualación, insertamos la función en la computación de tasas relativas de respuesta. Asumiendo que el refuerzo es el mismo para las dos opciones, el parámetro **k** se cancela y obtenemos la ecuación de igualación. 
 
$$\frac {R_1} {(R_1 + R_2)} = \frac {kr_1} {(kr_1 + kr_2)}$$

La función que postula proporcionalidad entre respuestas y refuerzos es consistente con los resultados presentados en la Fig x. Sin embargo, la función $$R_i = kr_i$$ propone que la tasa de una respuesta depende únicamente del refuerzo que ésta produce. Por lo tanto, la tasa de una respuesta no debería de cambiar cuando su refuerzo es constante y tan solo se manipula otra fuente de refuerzo. Sin embargo, en el panel derecho de la figura x observamos que la tasa de respuesta con el refuerzo constante decrece conforme incrementa el refuerzo para la respuesta alternativa: fenómeno que se conoce como **contraste conductual**.

La siguiente es una segunda función de respuesta, consistente con el muy replicado resultado de contraste conductual:

$$R_1 = \frac {kr_1} {(r_1 + r_2)}$$

La función nos dice que la tasa de una respuesta es una función de la tasa relativa de refuerzo que recibe esta respuesta. Como puede verse en el simulador, si la tasa de refuerzo $r_2$ para la otra opción es constante, la forma de la función de la tasa de respuesta $R_1$ es de ganancias decrecientes, igual a los datos empíricos reportados en la figura x. Por otra parte, si $r_1$ es constante, conforme incrementa el valor de $r_2$, la tasa de de respuesta $R_1$ decrementa, reproduciendo el patrón de contraste conductual.

Esta función es consistente con el resultado de la ley de igualación. Sustituyendo y cancelando el parámetro *k* y los denominadores, 
\begin{equation}
\frac{R_1}{R_1 + P_2} = \frac{\frac{kr_1}{r_1 + r_2}}{\frac{kr_1}{r_1 + r_2} + \frac{kr_2}{r_1 + r_2}} = \frac{kr_1}{kr_1 + kr_2} = \frac{r_1}{r_1 + r_2}
\end{equation}

Esta segunda función da cuenta de los resultados obtenidos en programas concurrentes. Sin embargo, veamos qué predice la misma función para experimentos en los que solo se refuerza una de las respuestas, como es el caso de los datos del experimento de Catania y Reynolds. En la ecuación:

 $$R_1 = \frac {kr_1} {(r_1 + r_2)}$$
 
el valor de $r_2$ es cero, pues solo hay una respuesta (r_1) y la ecuación se reduce a
 
  $$R_1 = \frac {kr_1} {r_1 }$$
  
Por lo tanto, terminamos con la función: 
  
  $$R_1 = k$$
  
la cual nos dice que la tasa de respuesta es una constante y que por ende, la respuesta es insensible a la tasa de refuerzo. Sin embargo, esta conclusión matemática no corresponde a los resultados reportados en la revisión de Villiers y Herrnstein sobre múltiples experimentos similares a los de Catania y Reynolds. En ella, se observa que para protocolos con una única opción de respuesta, dos cosas son ciertas: la tasa de respuesta NO crece linealmente en función de la tasa de refuerzo, contrario a lo que sugiere la primera función de Skinner; y, de igual manera, la tasa de respuesta TAMPOCO es una constante insensible a variaciones en la tasa de refuerzo, contrario a lo que sugiere la segunda función de refuerzo.

### La ley del Efecto Relativa

En un artículo publicado en 1970, Herrnstein propuso una versión de la ley del efecto que simultáneamente daba cuenta de dos de las regularidades empíricas más robustas: la igualación en programas concurrentes y la función de ganancias decrecientes entre tasa de respuesta y la tasa de refuerzo. Propuso un planteamiento que en su momento fue una gran salto: 

*Todo comportamiento es una instancia de una elección y el papel del refuerzo es redistribuir el comportamiento.*

La propuesta descansa en tres supuestos:

1. Los organismo están en constante actuar y no existen vacíos conductuales. En toda situación experimental hay al menos dos respuestas, una, $R_1$,  que medimos directamente (como picar una tecla, por ejemplo) y un conjunto de respuestas que no medimos directamente (como dar vueltas, aletear, picar el piso, etcétera) y que que Herrnstein llamó $R_o$. La suma de estas dos respuestas componen la totalidad del comportamiento *k*.

$$R_1 +Ro = k$$

2. En adición al refuerzo programado en una situación experimental, siempre hay otros refuerzos disponibles para el organismo, llamados *ro*.

3. Los organismos igualan la frecuencia relativa de la respuesta registrada y de todas las otras respuestas con la tasa relativa de refuerzo de todas las respuestas en una situación experimental.

$$\frac{R_1}{R_1 + R_0}= \frac {r_1} {(r_1 + r_o)}$$

$$R_1+R_0 = k$$

$$\frac{R_1}{k}= \frac {r_1} {(r_1 + r_o)}$$

$$R_1= \frac{kr_1} {(r_1 + r_o)}$$

Recordemos que $k$ es un parámetro que se estima estadísticamente y representa  la suma de todos los comportamientos medida en unidades $R_1$: frecuentemente respuestas por minuto. El valor de $k$ depende de la topografía (la forma específica) de la respuesta $R_1$ que se mide, así como del tiempo que toma ejecutarla. Picar una tecla es mucho más fácil para una paloma de lo que es apretar una palanca para una rata: como consecuencia, el valor de $k$ es mayor cuando se mide el comportamiento de picar una tecla que cuando se mide el comportamiento de apretar una palanca. 

$r_o$ es un segundo parámetro que se estima estadísticamente y que representa la tasa de otros refuerzos no controlados en el experimento. $r_o$ se mide en las mismas unidades que $r_1$: esto es, el número de refuerzos recibidos por unidad de tiempo.

En el simulador, ustedes pueden ver cómo varía la forma de la función que relaciona la tasa de respuesta a la tasa de refuerzo cuando se varía el valor de $r_o$ dentro de programas de intervalo variable. En general, estos modelos exponen cómo el número de refuerzos por unidad de tiempo generados por una respuesta (dentro de un programa de intervalo variable X) influye sobre la tasa con la que el organismo emite esa respuesta; al mismo tiempo, la tasa de refuerzo que generan las respuestas no controladas (R_o) influye sobre la tasa relativa de la respuesta evaluada (R_1).

* La función $R_1$ es de *ganancias decrecientes*, esto es, el impacto de un refuerzo adicional es **mayor** cuando  $r_1$ es pequeño y va **decreciendo** conforme $r_1$ incrementa. Ver figura.

* La función nos muestra que el impacto de un refuerzo contingente sobre una respuesta *depende del contexto de refuerzo*. A medida que aumenta $r_o$, el impacto de incrementar $r_1$ es menor. Con valores de $r_o$ muy pequeños, un contexto de refuerzo muy pobre, la tasa de respuesta es muy sensible a cambios en la tasa de refuerzo $r_1$ y justo lo opuesto ocurre con un contexto de refuerzo muy rico, donde $r_o$ es muy grande. Para una persona en pobreza extrema, pequeños cambios en el monto de su pensión tienen gran impacto en su comportamiento, pero no así para una persona en un entorno de riqueza. Ver figura.

### Impacto sobre la modificación de la conducta

La ley del efecto relativo ha tenido un gran impacto sobre la práctica de la modificación de la conducta. Considere un caso en el que se pretende reducir un comportamiento indeseable en un niño, como podría ser un comportamiento agresivo. De acuerdo a la versión original de la ley del efecto, una posibilidad es eliminar el refuerzo para ese comportamiento. Desafortunadamente, el refuerzo para la conducta agresiva del agresor puede ser la reacción de sumisión y respeto que este comportamiento induce en el niño agredido, sobre la cual no es fácil tener un control. Otra posibilidad es castigar al niño agresivo, arriesgando el surgimiento de otras respuestas indeseables. El modelo de Herrnstein proporciona una alternativa viable y exitosa: esta consiste en seleccionar y reforzar un comportamiento incompatible con el agresivo. Por ejemplo, se le pide a la maestra reforzar y jugar un juego de mesa con el niño. Para involucrarse plenamente en el juego de mesa, el niño tiene que sacrificar otros de sus comportamientos, entre ellos, sus comportamientos agresivos.

La estrategia puede extenderse a problemas tan severos como el consumo excesivo de bebidas alcohólicas. Una de las consecuencias del alcoholismo es el creciente aislamiento de la persona, con la asociada reducción en el contexto de refuerzo social. Esto genera un círculo vicioso, puesto que la reducción en el contexto de refuerzo social aumenta el impacto del refuerzo asociado con la bebida alcohólica. De acuerdo al modelo de Herrnstein, una estrategia exitosa sería recuperar la vida social de la persona, incrementando así los refuerzos que conforman su contexto cotidiano.

Similarmente, si se desea incrementar un comportamiento, la estrategia sugerida por el modelo de Herrnstein consiste en reducir las otras fuentes de refuerzo disponibles para la persona. El impacto de reforzar la conducta de estudio de un niño es mayor si se eliminan las opciones de la televisión y de un celular.  

En resumen, la ley del efecto relativa, al dirigir el estudio del comportamiento al estudio de la elección, proporciona una novedosa alternativa a las prácticas tradicionales de la modificación de la conducta. En lugar de simplemente eliminar o castigar conductas indeseables, se busca promover alternativas deseables y modificar el contexto de refuerzo para influir en las elecciones del individuo. 

### Evaluación

Empíricamente, la ley del efecto relativo de Herrnstein es enormemente exitosa en su descripción de la relación entre tasa de respuesta y la tasa de refuerzo dentro de programas de intervalo variable (de Villiers y Herrnstein). Sin embargo, la ecuación es algo más que un ejercicio para el ajuste de datos empíricos, esta es el resultado de un conjunto de supuestos teóricos que se reflejan en la interpretación de los dos parámetros de la ecuación $k$ y $r_o$. 

Las siguientes son algunas de las predicciones teóricas del modelo:

* La suma del total de respuestas, capturada por $k$, debe ser constante e independiente de variables que afecten el valor del refuerzo contingente y el valor de $r_o$, tales como manipulaciones motivacionales y la calidad del refuerzo contingente. En otras palabras, cuando se dan cambios en el valor de los refuerzos, lo que puede cambiar es la frecuencia relativa de las distintas respuestas, pero el número total de respuestas potencialmente realizables por el organismo debe permanecer constante. 
* Las manipulaciones en el tipo y la magnitud del refuerzo contingente deben ser capturadas por cambios en $r_o$.
* Agregar otra fuente de refuerzo debe reducir el valor del parámetro $r_o$.
 ### Quedamos de elaborar más/ejemplificar estos últimos puntos.. 

La evidencia acerca de los supuestos teóricos de la ley del efecto relativo no son consistentemente favorables (Dallery y Soto, 2004), en particular, en varios experimentos se reportan cambios en el parámetro $k$ cuando se dan cambios en la magnitud de refuerzo: es decir que no sólamente se redistribuye el número de respuestas asignado a las distintas opciones de comportamiento, sino que en efecto, el número total de respuestas que el organismo emite dentro de una cierta duración temporal incrementa. Estos resultados, acompañados del hecho de que la ecuación no se ha aplicado a los resultados obtenidos en programas de razón variable, han llevado a la derivación de la ecuación de ganancias decrecientes a partir de otros supuestos, tema que abordaremos en las siguientes notas.
Podemos concluir que, sin duda, la ley del efecto relativo de Herrnstein no solo generó una gran cantidad de evidencia empírica, sino que  adicionalmente brindó las bases para la gran mayoría de los modelos de acción. Estas bases serán desarrolladas a detalle posteriormente, pero pueden resumirse en la siguiente lista:

1. El estudio de la acción es el estudio de la elección.

2. La acción observada se mide por su tasa de ocurrencia y refleja la distribución total del comportamiento posible.

3. El efecto del refuerzo es cambiar la distribución del comportamiento.

4. El efecto del refuerzo depende del contexto de otros refuerzos presentes, incluyendo los refuerzos no medidos directamente.

5. La regla que gobierna la distribución del comportamiento es la igualación de la frecuencia relativa de la respuesta a la tasa relativa de refuerzo con la que ésta se encuentra asociada.

