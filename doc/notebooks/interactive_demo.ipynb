{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbouria/Notas-Aprendizaje-y-Comportamiento-Adaptable-I/blob/main/doc/notebooks/interactive_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR0h1HdvBVED"
      },
      "source": [
        "# Redes Neuronales online demo\n",
        "\n",
        "This is an interactive demo of a GDDM with leaky integration and exponentially collapsing bounds.\n",
        "\n",
        "This demo can be run like a normal Jupyter notebook.  If you've never used Jupyter notebooks before, hover over both headings below (\"Install PyDDM on Google Colab\" and \"Define the model and run the GUI\") and press the play button on each.  An interactive demo will show below.  To make changes to the model and try out your changes, click on \"Show code\" and edit it.  When you are done, click on the play button again to update the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OSvz7UeUtiiU"
      },
      "outputs": [],
      "source": [
        "#@title Install PyDDM on Google Colab\n",
        "!pip -q install git+https://github.com/mwshinn/PyDDM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LyBBVtpquqv2"
      },
      "outputs": [],
      "source": [
        "#@title Define the model and run the GUI\n",
        "import pyddm\n",
        "import pyddm.plot\n",
        "import numpy as np\n",
        "model = pyddm.gddm(drift=lambda x,leak,driftrate : driftrate - x*leak,\n",
        "                   noise=1,\n",
        "                   bound=lambda t,initial_B,collapse_rate : initial_B * np.exp(-collapse_rate*t),\n",
        "                   starting_position=\"x0\",\n",
        "                   parameters={\"leak\": (0, 2),\n",
        "                               \"driftrate\": (-3, 3),\n",
        "                               \"initial_B\": (.5, 1.5),\n",
        "                               \"collapse_rate\": (0, 10),\n",
        "                               \"x0\": (-.9, .9)})\n",
        "\n",
        "pyddm.plot.model_gui_jupyter(model)\n",
        "# pyddm.plot.model_gui(model) # If not using a Jupyter notebook or Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ilustraciones de Modelos Neuronales Simples en Python para Colab\n",
        "\n",
        "Estos ejemplos ilustran los conceptos básicos de los modelos neuronales vistos en las presentaciones, usando Python y NumPy. Puedes ejecutar cada celda de código de forma independiente.\n",
        "\n",
        "**Modelos:**\n",
        "1.  Perceptron (Umbral Lógico) para la función OR\n",
        "2.  Neurona Simple con Sigmoide y Descenso de Gradiente para OR\n",
        "3.  Red Multi-Capa (MLP) para la función XOR"
      ],
      "metadata": {
        "id": "2X-v7PryKYSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Perceptron (Umbral Lógico) para OR\n",
        "\n",
        "Este código implementa la idea básica de una neurona con una activación de umbral (sí/no) y la regla de aprendizaje del Perceptron. Intenta encontrar una línea que separe los casos (0,0) de los demás."
      ],
      "metadata": {
        "id": "akxMp9MaKs36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda para el Perceptron OR\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Función de activación de umbral (Step function)\n",
        "def step_function(x):\n",
        "  \"\"\"Devuelve 1 si x es >= 0, de lo contrario 0.\"\"\"\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "# --- Datos para la función OR ---\n",
        "# Entradas (X): Cada fila es un ejemplo [x1, x2]\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "# Salidas deseadas (Targets, T):\n",
        "targets = np.array([0, 1, 1, 1])\n",
        "\n",
        "# --- Inicialización del Perceptron ---\n",
        "# Pesos (W): Uno por cada entrada. Inicializamos pequeños valores aleatorios.\n",
        "# np.random.seed(42) # Descomentar para resultados reproducibles\n",
        "weights = np.random.rand(2) * 0.1 # Dos pesos para dos entradas\n",
        "# Sesgo (Bias, b): Un solo valor.\n",
        "bias = np.random.rand(1) * 0.1\n",
        "# Tasa de aprendizaje (learning rate, alpha): Qué tan grandes son los ajustes.\n",
        "learning_rate = 0.1\n",
        "# Número de épocas (pasadas completas por los datos)\n",
        "epochs = 50\n",
        "\n",
        "print(\"--- Perceptron para OR (Umbral Lógico) ---\")\n",
        "print(f\"Pesos iniciales: {weights}, Sesgo inicial: {bias}\")\n",
        "\n",
        "# --- Entrenamiento ---\n",
        "print(\"\\nIniciando entrenamiento...\")\n",
        "converged = False\n",
        "for epoch in range(epochs):\n",
        "  total_error = 0\n",
        "  for i in range(len(inputs)):\n",
        "    # 1. Calcular la entrada neta (producto punto + sesgo)\n",
        "    net_input = np.dot(inputs[i], weights) + bias\n",
        "\n",
        "    # 2. Obtener la predicción usando la función de activación\n",
        "    prediction = step_function(net_input)\n",
        "\n",
        "    # 3. Calcular el error (Target - Prediction)\n",
        "    error = targets[i] - prediction\n",
        "    total_error += abs(error) # Acumulamos el error absoluto para ver progreso\n",
        "\n",
        "    # 4. Actualizar pesos y sesgo (Regla de aprendizaje del Perceptron)\n",
        "    weights += learning_rate * error * inputs[i]\n",
        "    bias += learning_rate * error\n",
        "\n",
        "  # Imprimir error cada N épocas para ver el progreso\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "      print(f\"Época {epoch + 1}/{epochs}, Error total en la época: {total_error}\")\n",
        "  # Detener si no hay error\n",
        "  if total_error == 0 and not converged:\n",
        "      print(f\"Convergencia alcanzada en la época {epoch + 1}\")\n",
        "      converged = True\n",
        "      # break # Podemos detener o dejar que corra hasta el final\n",
        "\n",
        "if not converged:\n",
        "    print(\"Puede que no haya convergido completamente en las épocas dadas.\")\n",
        "\n",
        "print(f\"\\nPesos finales: {weights}, Sesgo final: {bias}\")\n",
        "\n",
        "# --- Prueba después del entrenamiento ---\n",
        "print(\"\\nPredicciones finales:\")\n",
        "for i in range(len(inputs)):\n",
        "  net_input = np.dot(inputs[i], weights) + bias\n",
        "  prediction = step_function(net_input)\n",
        "  print(f\"Entrada: {inputs[i]}, Predicción: {prediction}, Esperado: {targets[i]}\")\n"
      ],
      "metadata": {
        "id": "R9G4uL3dKutV",
        "outputId": "224111e1-0899-4130-86c8-29391ced1a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Perceptron para OR (Umbral Lógico) ---\n",
            "Pesos iniciales: [0.04613017 0.01186871], Sesgo inicial: [0.06341379]\n",
            "\n",
            "Iniciando entrenamiento...\n",
            "Convergencia alcanzada en la época 3\n",
            "Época 10/50, Error total en la época: 0\n",
            "Época 20/50, Error total en la época: 0\n",
            "Época 30/50, Error total en la época: 0\n",
            "Época 40/50, Error total en la época: 0\n",
            "Época 50/50, Error total en la época: 0\n",
            "\n",
            "Pesos finales: [0.04613017 0.11186871], Sesgo final: [-0.03658621]\n",
            "\n",
            "Predicciones finales:\n",
            "Entrada: [0 0], Predicción: 0, Esperado: 0\n",
            "Entrada: [0 1], Predicción: 1, Esperado: 1\n",
            "Entrada: [1 0], Predicción: 1, Esperado: 1\n",
            "Entrada: [1 1], Predicción: 1, Esperado: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neurona Simple con Sigmoide y Descenso de Gradiente para OR\n",
        "\n",
        "Este modelo usa la función sigmoide (salida continua entre 0 y 1) y ajusta los pesos usando Descenso de Gradiente para minimizar el error cuadrático. La salida puede interpretarse como una probabilidad."
      ],
      "metadata": {
        "id": "poH04qWYLYAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda para Neurona Sigmoide OR\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Función de activación Sigmoide\n",
        "def sigmoid(x):\n",
        "  \"\"\"Calcula la función sigmoide.\"\"\"\n",
        "  # Clip para evitar overflow/underflow en np.exp\n",
        "  x_clipped = np.clip(x, -500, 500)\n",
        "  return 1 / (1 + np.exp(-x_clipped))\n",
        "\n",
        "# Derivada de la función sigmoide (necesaria para gradiente)\n",
        "def sigmoid_derivative(x):\n",
        "  \"\"\"Calcula la derivada de la sigmoide.\"\"\"\n",
        "  sig = sigmoid(x)\n",
        "  return sig * (1 - sig)\n",
        "\n",
        "# --- Datos para la función OR ---\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "targets = np.array([[0], [1], [1], [1]]) # Usamos [[],[]] para consistencia dimensional\n",
        "\n",
        "# --- Inicialización ---\n",
        "# np.random.seed(42) # Descomentar para resultados reproducibles\n",
        "weights = np.random.rand(2, 1) * 0.1 # Forma (2, 1) para 2 entradas, 1 salida\n",
        "bias = np.random.rand(1) * 0.1\n",
        "learning_rate = 0.5 # Puede necesitar ajuste\n",
        "epochs = 10000 # Generalmente necesita más épocas que el Perceptron simple\n",
        "\n",
        "print(\"\\n--- Neurona Sigmoide para OR (Descenso Gradiente) ---\")\n",
        "print(f\"Pesos iniciales:\\n{weights}\\nSesgo inicial: {bias}\")\n",
        "\n",
        "# --- Entrenamiento ---\n",
        "print(\"\\nIniciando entrenamiento...\")\n",
        "for epoch in range(epochs):\n",
        "  # Propagación hacia adelante (Forward Pass) - Todos los ejemplos a la vez\n",
        "  net_input = np.dot(inputs, weights) + bias\n",
        "  predictions = sigmoid(net_input)\n",
        "\n",
        "  # Cálculo del error\n",
        "  error = targets - predictions\n",
        "\n",
        "  # Cálculo del gradiente (Regla Delta Generalizada)\n",
        "  # Gradiente = error * derivada_activacion * entrada_transpuesta\n",
        "  delta = error * sigmoid_derivative(net_input)\n",
        "\n",
        "  # Ajuste de pesos y sesgo\n",
        "  weights += learning_rate * np.dot(inputs.T, delta) # inputs.T alinea dimensiones\n",
        "  bias += learning_rate * np.sum(delta, axis=0) # Sumamos ajustes para todas las muestras\n",
        "\n",
        "  # Imprimir error (ECM) cada N épocas\n",
        "  if (epoch + 1) % 1000 == 0:\n",
        "    loss = np.mean(np.square(error)) # Calculamos el Error Cuadrático Medio\n",
        "    print(f\"Época {epoch + 1}/{epochs}, Error Cuadrático Medio: {loss:.6f}\")\n",
        "\n",
        "print(f\"\\nPesos finales:\\n{weights}\\nSesgo final: {bias}\")\n",
        "\n",
        "# --- Prueba después del entrenamiento ---\n",
        "print(\"\\nPredicciones finales:\")\n",
        "# Hacemos una pasada final con los pesos entrenados\n",
        "net_input = np.dot(inputs, weights) + bias\n",
        "predictions = sigmoid(net_input)\n",
        "for i in range(len(inputs)):\n",
        "  # Redondeamos para ver la clasificación binaria\n",
        "  pred_class = 1 if predictions[i][0] >= 0.5 else 0\n",
        "  print(f\"Entrada: {inputs[i]}, Predicción: {predictions[i][0]:.4f} (Clase: {pred_class}), Esperado: {targets[i][0]}\")\n"
      ],
      "metadata": {
        "id": "zk8kJuaELfiD",
        "outputId": "3cc673d1-3e7c-4fa9-ad62-45ac337c9d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Neurona Sigmoide para OR (Descenso Gradiente) ---\n",
            "Pesos iniciales:\n",
            "[[0.07863678]\n",
            " [0.00149292]]\n",
            "Sesgo inicial: [0.06227541]\n",
            "\n",
            "Iniciando entrenamiento...\n",
            "Época 1000/10000, Error Cuadrático Medio: 0.002906\n",
            "Época 2000/10000, Error Cuadrático Medio: 0.001349\n",
            "Época 3000/10000, Error Cuadrático Medio: 0.000869\n",
            "Época 4000/10000, Error Cuadrático Medio: 0.000639\n",
            "Época 5000/10000, Error Cuadrático Medio: 0.000504\n",
            "Época 6000/10000, Error Cuadrático Medio: 0.000416\n",
            "Época 7000/10000, Error Cuadrático Medio: 0.000354\n",
            "Época 8000/10000, Error Cuadrático Medio: 0.000307\n",
            "Época 9000/10000, Error Cuadrático Medio: 0.000272\n",
            "Época 10000/10000, Error Cuadrático Medio: 0.000244\n",
            "\n",
            "Pesos finales:\n",
            "[[7.94183944]\n",
            " [7.94183914]]\n",
            "Sesgo final: [-3.73488481]\n",
            "\n",
            "Predicciones finales:\n",
            "Entrada: [0 0], Predicción: 0.0233 (Clase: 0), Esperado: 0\n",
            "Entrada: [0 1], Predicción: 0.9853 (Clase: 1), Esperado: 1\n",
            "Entrada: [1 0], Predicción: 0.9853 (Clase: 1), Esperado: 1\n",
            "Entrada: [1 1], Predicción: 1.0000 (Clase: 1), Esperado: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Red Multi-Capa (MLP) para XOR\n",
        "\n",
        "El problema XOR no es linealmente separable, por lo que requiere una red con al menos una capa oculta. Este código implementa una MLP simple y usa retropropagación (Backpropagation) para ajustar los pesos de todas las capas."
      ],
      "metadata": {
        "id": "Bx91CqKtL0eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Funciones sigmoide y su derivada (igual que antes)\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-np.clip(x, -500, 500))) # Añadido clip para estabilidad\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  sig = sigmoid(x)\n",
        "  return sig * (1 - sig)\n",
        "\n",
        "# --- Datos para la función XOR ---\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "targets = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# --- Arquitectura de la Red ---\n",
        "input_neurons = 2\n",
        "hidden_neurons = 3 # Podemos probar con 2 o 3 neuronas ocultas\n",
        "output_neurons = 1\n",
        "\n",
        "# --- Inicialización ---\n",
        "# np.random.seed(42)\n",
        "# Pesos y sesgos para la capa oculta\n",
        "weights_ih = np.random.rand(input_neurons, hidden_neurons) * 0.1\n",
        "bias_h = np.random.rand(1, hidden_neurons) * 0.1\n",
        "# Pesos y sesgos para la capa de salida\n",
        "weights_ho = np.random.rand(hidden_neurons, output_neurons) * 0.1\n",
        "bias_o = np.random.rand(1, output_neurons) * 0.1\n",
        "\n",
        "learning_rate = 0.1 # XOR puede ser sensible a la tasa\n",
        "epochs = 30000\n",
        "\n",
        "print(\"\\n--- Red Multi-Capa (MLP) para XOR ---\")\n",
        "print(f\"Arquitectura: {input_neurons} (Entrada) -> {hidden_neurons} (Oculta) -> {output_neurons} (Salida)\")\n",
        "\n",
        "# --- Entrenamiento con Backpropagation ---\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  # Usaremos un bucle explícito para cada ejemplo para ver los pasos de backprop\n",
        "  final_predictions = [] # Para guardar predicciones de la época\n",
        "\n",
        "  for i in range(len(inputs)):\n",
        "    input_layer = inputs[i:i+1] # Tomamos una fila (ejemplo) a la vez\n",
        "    target_output = targets[i:i+1]\n",
        "\n",
        "    # --- 1. Propagación hacia adelante (Forward Pass) ---\n",
        "    # Capa Oculta\n",
        "    hidden_layer_input = np.dot(input_layer, weights_ih) + bias_h\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    # Capa de Salida\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_ho) + bias_o\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "    final_predictions.append(predicted_output[0][0])\n",
        "\n",
        "    # --- 2. Cálculo del Error (en la salida) ---\n",
        "    output_error = target_output - predicted_output\n",
        "    total_loss += np.mean(np.square(output_error)) # Acumulamos ECM\n",
        "\n",
        "    # --- 3. Retropropagación del Error (Backward Pass) ---\n",
        "    # Gradiente para la capa de salida\n",
        "    d_predicted_output = output_error * sigmoid_derivative(output_layer_input)\n",
        "\n",
        "    # Error contribuido por la capa oculta (propagado hacia atrás)\n",
        "    hidden_layer_error = np.dot(d_predicted_output, weights_ho.T)\n",
        "\n",
        "    # Gradiente para la capa oculta\n",
        "    d_hidden_layer = hidden_layer_error * sigmoid_derivative(hidden_layer_input)\n",
        "\n",
        "    # --- 4. Actualización de Pesos y Sesgos ---\n",
        "    # Capa de Salida\n",
        "    weights_ho += learning_rate * np.dot(hidden_layer_output.T, d_predicted_output)\n",
        "    bias_o += learning_rate * np.sum(d_predicted_output, axis=0, keepdims=True)\n",
        "\n",
        "    # Capa Oculta\n",
        "    weights_ih += learning_rate * np.dot(input_layer.T, d_hidden_layer)\n",
        "    bias_h += learning_rate * np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "  # Imprimir error promedio cada N épocas\n",
        "  if (epoch + 1) % 2000 == 0:\n",
        "    avg_loss = total_loss / len(inputs)\n",
        "    print(f\"Época {epoch + 1}/{epochs}, Error Cuadrático Medio: {avg_loss:.6f}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Prueba final MLP para XOR ---\")\n",
        "# Hacemos una pasada final con todos los inputs para mostrar resultados\n",
        "hidden_layer_input = np.dot(inputs, weights_ih) + bias_h\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_ho) + bias_o\n",
        "final_predictions_all = sigmoid(output_layer_input)\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "    pred_class = 1 if final_predictions_all[i][0] >= 0.5 else 0\n",
        "    print(f\"Entrada: {inputs[i]}, Predicción: {final_predictions_all[i][0]:.4f} (Clase: {pred_class}), Esperado: {targets[i][0]}\")\n"
      ],
      "metadata": {
        "id": "OmzDRYFiMBRl",
        "outputId": "97cf61a3-ad94-4df8-ee28-c16ca8ea6e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Red Multi-Capa (MLP) para XOR ---\n",
            "Arquitectura: 2 (Entrada) -> 3 (Oculta) -> 1 (Salida)\n",
            "Época 2000/30000, Error Cuadrático Medio: 0.252699\n",
            "Época 4000/30000, Error Cuadrático Medio: 0.252553\n",
            "Época 6000/30000, Error Cuadrático Medio: 0.252430\n",
            "Época 8000/30000, Error Cuadrático Medio: 0.252326\n",
            "Época 10000/30000, Error Cuadrático Medio: 0.252230\n",
            "Época 12000/30000, Error Cuadrático Medio: 0.252116\n",
            "Época 14000/30000, Error Cuadrático Medio: 0.251845\n",
            "Época 16000/30000, Error Cuadrático Medio: 0.248412\n",
            "Época 18000/30000, Error Cuadrático Medio: 0.187177\n",
            "Época 20000/30000, Error Cuadrático Medio: 0.049949\n",
            "Época 22000/30000, Error Cuadrático Medio: 0.009068\n",
            "Época 24000/30000, Error Cuadrático Medio: 0.004347\n",
            "Época 26000/30000, Error Cuadrático Medio: 0.002775\n",
            "Época 28000/30000, Error Cuadrático Medio: 0.002014\n",
            "Época 30000/30000, Error Cuadrático Medio: 0.001571\n",
            "\n",
            "--- Prueba final MLP para XOR ---\n",
            "Entrada: [0 0], Predicción: 0.0341 (Clase: 0), Esperado: 0\n",
            "Entrada: [0 1], Predicción: 0.9623 (Clase: 1), Esperado: 1\n",
            "Entrada: [1 0], Predicción: 0.9623 (Clase: 1), Esperado: 1\n",
            "Entrada: [1 1], Predicción: 0.0477 (Clase: 0), Esperado: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "interactive_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}