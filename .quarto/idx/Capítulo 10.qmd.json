{"title":"Comportamiento de Elección: Maximización Local","markdown":{"yaml":{"title":"Comportamiento de Elección: Maximización Local ","format":"html"},"headingText":"Arturo Bouzas","containsRefs":false,"markdown":"\n\n\nEn el capítulo anterior, vimos que estudiando una medida agregada de respuestas y refuerzos, en equilibrio, la tasa relativa de respuestas tiende a igualar a la tasa de refuerzo que produce cada opción. Contemplamos dos posibles explicaciones computacionales para este fenómeno: la igualación de la probabilidad de refuerzo para las opciones de respuesta (igualación de la rentabilidad de las respuestas) y la maximización de la tasa global de refuerzo. \n\nUna alternativa a estos modelos molares son los modelos de *maximización local*, los que asumen que igualación es el resultado de que en cada oportunidad de respuesta, los organismos eligen aquella respuesta asociada con el valor más alto de alguna variable local. Vamos a revisar tres miembros de esta familia de modelos, que se distinguen entre ellos por la variable local que cada uno maximiza:\n\n* *Maximización Momentánea*: Modelos de elección basados en las probabilidades instantáneas de refuerzo de cada alternativa.\n* *Mejoramiento*: Modelos de elección basados en las tasas locales de refuerzo asociadas con cada alternativa.\n* *Valor Q de la respuesta*.\n\n## Maximización Momentánea\n\nEste modelo asume que los organismos computan las probabilidades locales de refuerzo asociadas con cada respuesta. Consideren un programa concurrente IV - IV. En esos programas, la probabilidad de un refuerzo para una respuesta en cada tecla es una función del tiempo transcurrido desde la última visita a una de ellas. Conforme incrementa el tiempo de estancia en una opción, incrementa la probabilidad de un refuerzo para la respuesta en la opción alterna. Este modelo fue propuesto por Shimp en 1969, quien argumentó que el cambio de una opción a otra era controlado por los cambios en la probabilidad de refuerzo asociados con el tiempo transcurrido en cada opción.  Para él, la igualación global era el resultado de una regla de respuesta seguida por el organismo en la cual este respondía a la tecla que tuviera la mayor probabilidad de refuerzo al momento de la elección. Simulando este modelo, Shimp encontró que a nivel global este algoritmo resulta efectivamente en tasas de respuesta iguales a las tasas de refuerzo: reproduciendo así el patrón de igualación.\n\n### Evaluación experimental del modelo de maximización momentánea\n\n#### 1. Programas de refuerzo concurrente IV - IV de ensayos discretos\n\nEmpíricamente, el modelo de maximización instantánea predice regularidades en la estructura de los cambios del organismo de una opción a otra después de ciertas secuencias de respuesta. Si se computan las probabilidades de refuerzo de cada una de las opciones de respuesta, es posible determinar cuál respuesta debe seguir después de una secuencia de *n* respuestas consecutivas en una misma opción. La forma más accesible de estudiar esta predicción es empleando procedimientos de programas de refuerzo concurrentes IV - IV, pero de ensayos discretos. En estos programas, al animal se le presenta la oportunidad de elegir entre dos alternativas con una sola respuesta. Las dos opciones se presentan durante un breve periodo de tiempo. Después de una respuesta a alguna de las dos alternativas, inicia un breve intervalo entre ensayos sin opción de respuesta: al final de este intervalo, al organismo se le presenta una vez más una breve oportunidad para elegir entre una de las dos respuestas. Lo que es importante tener presente frente a estos protocolos es que los programas de refuerzo siguen corriendo durante los intervalos entre ensayos como si no hubiese discontinuidades en el tiempo. Al igual que en los programas IV - IV que hemos revisado, una vez que un refuerzo está disponible para una de las opciones, el reloj del programa IV se detiene y el refuerzo se guarda hasta que es recogido por el animal. Con este protocolo, mientras mayor es el número de elecciones repetidas por una de las opciones, mayor es la probabilidad de que un refuerzo esté esperando en la opción alterna. \n\nEn experimentos con programas de ensayos discretos, se mide la probabilidad de cambiar de alternativa después de varias secuencias de respuesta a una misma tecla. Mientras mayor sea el número de respuestas seguidas a una misma tecla, mayor será la probabilidad de refuerzo asociada con la respuesta alterna. De acuerdo al modelo de maximización instantánea, debería observarse que la probabilidad de cambiar de alternativa es una función del número de respuestas que se hayan dado a la misma tecla. Por ejemplo, si el programa es uno concurrente de tecla verde con IV 1’ - tecla roja con IV 3’, el modelo predice la siguiente secuencia de tres respuestas: Verde-Verde-Roja. Por los valores de los programas de intervalo, después de cada dos respuestas seguidas a la opción verde resulta más probable que se otorgue un refuerzo a la opción roja.\n\nEn el experimento más citado con este procedimiento, Nevin () no encontró evidencia en favor del modelo de maximización instantánea. Ver Figuras. \n\n![](media/17201205364798/17201396263913.jpg)\n\nObsérvese en la figura del panel izquierdo que la probabilidad de refuerzo de la Opción Roja aumenta con la acumulación de elecciones de la Opción Verde por parte del organismo. Al mismo tiempo, en la figura del panel derecho, nótese que la probabilidad de cambiar a la Opción Roja no aumenta con la acumulación de elecciones de la Opción Verde por parte del organismo. Nota: la probabilidad de cambiar de opción se calculó con base al número de oportunidades para cambiar de opción en cada secuencia (run length). Así, estos resultados rompen con el patron de elecciones esperado según el modelo de maximización instantánea. \n\nEn un análisis posterior de su experimento original  y de los datos de un experimento de Silberberg (), Nevin () encontró que la perseverancia en las opciones de respuesta (independientemente de su probabilidad de refuerzo) era el patrón más frecuentemente observado en ambos experimentos. Los animales tendían a quedarse en la tecla a la que habían respondido anteriormente y no a cambiar como una función del número de respuestas a esa tecla Ver fig.\n\n![](media/17201205364798/17201399029035.jpg)\n\n#### 2. Programas de refuerzo concurrente RV - IV de ensayos discretos\n\nUn protocolo adicional para evaluar el modelo de maximización local es un programa concurrente de ensayos discretos RV - IV. En estos programas, la probabilidad de refuerzo para las respuestas asociadas a la opción RV es constante, mientras que la probabilidad de refuerzo para la respuesta asociada al programa IV cambia como una función de la última respuesta a esa opción. Una estrategia consistente con el modelo de maximización instantánea consiste en responder a la opción asociada con el programa RV inmediatamente después de recibir un refuerzo en la opción asociada con el programa IV. Este es el momento con menor probabilidad de refuerzo para la opción IV. De igual forma, la probabilidad de un cambio de la opción RV hacia la opción IV debe incrementar como una función del número de respuestas que se han dado a la opción RV. \n\nLa siguiente figura x presenta los resultados obtenidos por Williams () usando el protocolo anterior. En primer lugar, Williams encontró que los animales igualaban la frecuencia relativa de respuestas a la frecuencia relativa de refuerzos, pero de manera aún más importante para estas notas: no encontró evidencia de que la respuesta fuese controlada por la probabilidad instantánea de refuerzo. El panel izquierdo de la figura muestra la probabilidad de refuerzo como una función del número de ensayos desde la última respuesta a la opción IV. Puede verse que la probabilidad de refuerzo para un cambio a la tecla IV es creciente; consecuentemente, la probabilidad de una respuesta a la tecla IV también debería incrementar como una función del número de ensayos desde la última respuesta a la opción IV. Sin embargo, el panel derecho de la figura muestra que la probabilidad real u observada de una respuesta al IV es constante o decreciente.\n\n![](media/17201205364798/17201421458762.jpg)\n\n\n### Conclusiones acerca del modelo de maximización Instantánea\n\nDe los experimentos con el protocolo de ensayos discretos recién descritos y muchos otros que no presentamos se pueden alcanzar las siguientes conclusiones:\n\n* Consistente con la ley del efecto, la probabilidad de que se repita una opción de respuesta incrementa si esta es seguida por un refuerzo. \n* También se observa un *efecto de perseverancia*: es más probable que se repita la respuesta a una opción que ya fue elegida en el pasado, independientemente de si esta respuesta es reforzada o no. \n* Es importante agregar que la consistencia de los resultados descritos también depende de otras variables como el intervalo entre opciones de elección, el cual afecta la memoria del organismo sobre sus elecciones previas.\n\n## Modelo de Mejoramiento\n\nEl modelo de mejoramiento de Herrnstein y Vaughan () describe la dinámica de elección en situaciones de elección recurrentes, momento a momento. Como otros modelos de elección basados en el valor de las opciones, el modelo de mejoramiento cuenta con dos componentes: \n\n* El primer componente es la especificación de la variable de decisión a partir de la cual el organismo elige (es decir, la variable que guía la elección de los organismos; ejemplos de estas variables son la probabilidad de refuerzo, el tiempo transcurrido en una opción o las tasas locales de refuerzo, entre otras opciones).\n\n* El segundo componente es *la regla de respuesta*: esto es, con qué criterio elige el organismo.\n\nPara el modelo de mejoramiento, la variable de decisión son las tasas locales de refuerzo, a las cuales previamente llamamos como la rentabilidad de las opciones. Esta variable se computa dividiendo el número de refuerzos para cada opción entre el tiempo asignado a cada una de ellas:\n\n$$\\frac {r_i}{T_i}$$\n\nLa rentabilidad puede computarse también para respuestas:\n\n$$\\frac {r_i}{R_i}$$\n\nAsumiendo un tiempo total fijo, de acuerdo al modelo de mejoramiento, cada incremento en una unidad de tiempo asignada a una opción tiene como consecuencia la actualización de las dos tasas de refuerzo locales. Al incrementar el tiempo asignado $t_i$ a una de las opciones, la tasa local de refuerzo de esa opción disminuye (dado que el denominador de la rentabilidad de esa opción crece); simultáneamente, este cambio también reduce el tiempo $t_2$ asignado a la otra opción (dado que el tiempo total es fijo), lo cual incrementa la tasa de refuerzo local asociada con la segunda opción (puesto que el denominador de esta segunda rentabilidad se achica).  \n\nEl segundo componente del modelo de mejoramiento, la regla de elección, es una variante de maximización que consiste en seleccionar la opción con la mejor tasa de refuerzo local dentro de cada oportunidad.\n\nEste modelo describe la elección dentro de un programa concurrente que se construye como un sistema dinámico y de retroalimentación. Es decir, bajo este arreglo, la opción de respuesta que tiene la mayor tasa local de refuerzo va cambiando como una función de la asignación de tiempos y respuestas por parte del organismo. Es por ello que este modelo puede dar cuenta de la dinámica global de acercamiento del organismo al patrón de igualación observado en programas concurrentes IV - IV. Para entender al modelo, tengan presente que al elegir la opción con la mejor tasa de refuerzo local, el organismo está incrementando el tiempo de estancia en esa opción: lo cual aumenta la base temporal *t* para computar la tasa de refuerzo local de esa opción. Simultáneamente, y dado el tiempo total fijo, este comportamiento reduce el tiempo de estancia en la segunda opción, lo cual incrementa la tasa local de refuerzo de esa segunda opción. De esta forma, cada aumento de una unidad de tiempo de estancia en la mejor opción por parte del organismo, tiene dos consecuencias: reduce la tasa de refuerzo local de esa opción e incrementa la tasa de refuerzo local de la otra opción. La transición del organismo de una opción a la otra ocurre cuando la dirección de la diferencia entre las dos tasas cambia de signo y, básicamente, la opción alterna se vuelve mejor que la opción actual. Pueden observar que en estos programas, el modelo de mejoramiento es uno de retroalimentación y corresponde a un algoritmo cuyo blanco es la reducción de la diferencia entre las dos tasas de refuerzo locales; el seguimiento de este algoritmo a la larga (o en equilibrio) resulta en la igualación global de las tasas de respuesta a las tasas de refuerzo para ambas opciones (el patrón de igualación).\nConsideren como ejemplo un programa concurrente IV 1 min - IV 2 min. Supongan que al inicio de la sesión de una hora, el organismo asigna la mitad de su tiempo a cada alternativa.\nSupuesto importante: Vamos a asumir que el animal responde a una tasa moderada que garantiza que en ambos programas de IV el organismo obtendrá el máximo número de reforzadores posibles (para 1 hora), independientemente del tiempo que efectivamente dedique a cada opción durante la sesión. Esto ocurre porque en los programas de intervalo variable, los reforzadores se \"acumulan\" durante el tiempo que el animal no está respondiendo a esa opción. Por lo tanto, aunque el animal dedique menos de 1 hora a una opción, si este responde de manera suficientemente rápida, efectivamente podría obtener todos los reforzadores correspondientes a 1 hora pero en una menor cantidad de tiempo.\nDado este supuesto, las tasas de refuerzo locales para las dos opciones se calculan como el número máximo posible de refuerzos en una hora dividido por la proporción de tiempo realmente asignada a cada opción. Así, la tasa local de refuerzo para la tecla IV 1' sería de 60 reforzadores máximos / 0.5 hr = 120 refuerzos por hora. Una vez más, esto significa que aunque el animal solo dedica media hora a esta opción, este puede obtener los 60 reforzadores disponibles durante la sesión completa gracias a la acumulación propia de los programas IV. Nótese que si el organismo le asigna toda la hora a responder a este programa, su tasa local de refuerzo será de 60/1 hr = 60 refuerzos por hora. Al reducir el tiempo asignado a esa opción a media hora (0.5 hr), la rentabilidad de esa opción aumenta a 120 refuerzos por hora, aunque el número absoluto de reforzadores (60) permanece constante. Para el IV 2', la tasa local de refuerzo es de 30 reforzadores máximos / 0.5 hr = 60 refuerzos por hora.\nDados estos resultados y de acuerdo al modelo de mejoramiento, el animal debería escoger asignar más tiempo a la opción con el programa IV 1' que a la opción con el programa IV 2’, ya que la primera presenta una mayor tasa de refuerzo local (120 vs 60).\nSupongan ahora que como resultado de asignar más tiempo a la opción IV 1', el organismo termina dedicando el 90% de su tiempo a esta opción. Ahora, para la opción asociada con el IV 1 min, la tasa de refuerzo local sería de 60/0.9 hr = 66.7 refuerzos por hora. En el IV 2 min, la tasa de refuerzo local sería de 30/0.1 hr = 300 refuerzos por hora. Recordamos que los programas en sí no han cambiado: lo único que está modificando la rentabilidad percibida de ambas opciones es la manera en la que el organismo distribuye su tiempo entre ellas. \nPueden observar que al incrementar el tiempo asignado al IV 1', paradójicamente se incrementó la tasa local de refuerzo asociada con la opción IV 2' (de 60 a 300 refuerzos por hora) y por lo tanto, si el organismo sigue la regla de mejoramiento, su subsecuente elección debe ser la opción del IV 2'.\nBajo este modelo, el equilibrio se alcanza cuando la proporción de tiempo asignado a las dos opciones por parte del organismo es de 2/3 para IV 1' (60/0.666  90) y 1/3 para IV 2' (30/0.333  90). Lo anterior es aproximadamente igual a \"90 refuerzos por hora\" en ambas alternativas. Así, en el punto de equilibrio, ambas alternativas ofrecen exactamente la misma tasa de refuerzo local (90 por hora), lo que explica por qué el organismo deja de cambiar su distribución de tiempo entre las opciones tras alcanzar este punto. Este resultado es equivalente a la **igualación** global de tasas relativas de respuesta a las tasas relativas de refuerzo: \n0.66 hr/(0.66 hr + 0.33 hr)  60 r/(60 r + 30 r)\nEn el correspondiente simulador, ustedes podrán ver la dinámica del sistema para diferentes valores de programas concurrentes.\n\nUn problema importante del modelo de mejoramiento es que deja sin especificar la ventana temporal que los organismos requieren para computar las tasas de refuerzo locales de las distintas opciones. ¿Estas tasas se estiman hasta finalizar la duración de cada sesión experimental? O bien aún, ¿seguirá el organismo algoritmos menos evidentes? Por ejemplo, ¿reiniciar la computación de las tasas para las dos opciones cada 10 minutos?¿O borrar la historia con las opciones de respuesta experimentadas días atrás? Las respuestas a todas estas preguntas no son evidentes. Y al mismo tiempo, estas tienen importantes implicaciones para la aplicación del modelo a entornos volátiles, los cuales frecuentemente cambian las condiciones de refuerzo para las diferentes opciones a lo largo del tiempo. Este tema será abordado en otras notas.\n\n## Modelo de Valor *Q* de la respuesta\n\nDe los modelos que dan cuenta de la elección recurrente, el modelo de valor *Q* que vimos en las notas x es el más cercano a la ley del efecto original planteada por Thorndike. La variable de decisión de este modelo es el valor *Q* adquirido por cada opción de respuesta. Este valor representa la integración de la historia de reforzamiento de cada opción que resulta de la regla del error de predicción. Por otra parte, la regla de respuesta bajo este modelo consiste en la elección probabilística de la respuesta con mayor valor *Q* en cada oportunidad. Presten atención a que en este modelo, cada respuesta adquiere su valor exclusivamente como función de los refuerzos que produce y es independiente del valor de las demás alternativas presentes.\n\nPor consecuente, una predicción importante de este modelo es que cuando al organismo se le presenta la oportunidad de elegir entre dos respuestas con diferentes valores *Q*, su elección debe ser independiente del contexto donde fue adquirido el valor *Q* de cada opción. Imaginen que en su ciudad hay tres cadenas de cafeterías (A, B y C). Cerca de su casa y cerca de su trabajo, hay dos sucursales disponibles (digamos que A y B están cerca de su casa; y B y C están cerca de su trabajo). Una de las cadenas de cafetería, la B, tiene una sucursal en los dos escenarios donde ustedes compran café (a la sucursal cerca de su casa le llamaremos B’ y a la sucursal cerca de su trabajo le llamaremos B’’). Las sucursales de la cadena B tienen un logo que las distingue. Digamos que sus menús son similares, por lo que las dos sucursales B, aunque tienen sutiles diferencias en cuanto a su staff, comparten a grandes rasgos el mismo programa de recompensas (B’ = B’’). En cambio, las cafeterías A y C son claramente distinguibles en cuanto a calidad con relación a ambas cafeterías B. En resumen: A es mucho mejor que B (A > B) y C es mucho peor que B (C < B). Sin embargo, un domingo ustedes acuden a otra zona de la ciudad y se dan cuenta de que las dos sucursales de las cafeterías B (B’ y B’’) han sido reubicadas ahora en una misma y nueva zona de la ciudad. Dado que los programa de recompensas de ambas opciones son iguales (B’ = B’’), ustedes deberían ser indiferentes entre ellas. Sin embargo, es posible que los valores percibidos de B’ y de B’’ dependan de cuál era el otro restaurante con el que ambas opciones competían dentro de su contexto previo (A o C).  \nUna forma de evaluar experimentalmente esta predicción sobre la relevancia del contexto previo de los refuerzos es presentar a los animales con dos situaciones de elección diferentes:\nPrograma concurrente 1: El animal puede elegir entre:\nOpción A: reforzada con intervalo variable IV(a)\nOpción B: reforzada con intervalo variable IV(b)\nPrograma concurrente 2: El animal puede elegir entre:\nOpción A: reforzada con el mismo intervalo variable IV(a) que en la Situación 1\nOpción B': reforzada con un intervalo variable IV(b') diferente al de la Situación 1\nPosteriormente, se evalúa la preferencia del animal entre las opciones A de ambos programas concurrentes, las cuales comparten exactamente el mismo valor de reforzamiento (Q). Bajo este protocolo, cada par de opciones (cada componente o programa concurrente) se encuentra vigente en períodos de tiempo separados dentro de una misma sesión (formalmente a este arreglo se le conoce como programas múltiples de refuerzo). Una vez alcanzado el equilibrio en cada programa concurrente IV - IV: se presenta una nueva combinación de las opciones comunes a ambos programas. De acuerdo a los modelos de refuerzo, de mejoramiento y de valor Q, la elección en los periodos de prueba debe reflejar los valores adquiridos por cada opción individual.\n\nBelke () corrió una versión de este protocolo. Para uno de los componentes de programas concurrentes (Componente A), los valores de las opciones eran Tecla Blanca (IV 20) vs Tecla Roja (IV 40), mientras que para el otro programa concurrente (el Componente B), las opciones eran Tecla Verde con IV 40 vs una Tecla Amarilla con IV 80. Así, para el Componente A, la opción del IV 40 ocurría en el contexto de una opción IV 20 (la Tecla Roja) que era dos veces más rica. Mientras tanto, para el segundo par de estímulos (el Componente B), la opción IV 40 (la Tecla Verde) se presentaba en el contexto de otra opción reforzada con un IV 80 (la Tecla Amarilla), la cual era dos veces menos rica. De acuerdo al modelo *Q* que no considera el contexto de los reforzadores, las dos opciones de respuesta reforzadas con el IV 40 (las Teclas Roja y Verde) en ambos programas deben tener el mismo valor. Sin embargo, otra posibilidad es que el valor adquirido por una opción de respuesta dependa del contexto de los refuerzos proporcionados a las respuestas alternativas presentes. En este caso, la Tecla Verde reforzada con el IV 40 dentro del contexto de otra opción reforzada con un IV pobre, adquiere un valor más grande que la Tecla Roja, también reforzada con un IV 40 pero que fue entrenada previamente en un contexto con una opción reforzada con un IV más rico. \n\nContrario a la predicción de indiferencia arrojada por el modelo de refuerzo *Q*, Belke encontró que los animales sí preferían la opción IV 40 entrenada en el contexto pobre, por encima de la misma opción entrenada en el contexto rico. Esta evidencia sugiere que el impacto de los reforzadores sobre el valor de una respuesta depende de los reforzadores obtenidos por las otras opciones presentes en el pasado (es decir, con qué otros estímulos ha convivido cada estímulo previamente). \n\nLos resultados anteriores son consistentes con la siguiente interpretación más local de la ejecución en programas concurrentes. La interpretación consiste en suponer que en estos programas lo que el animal aprende es el tiempo que debe pasar en una opción antes de cambiar a otra alternativa. Suponemos que estos tiempos se encuentran relacionados linealmente a la tasa de refuerzo en la tecla alternativa: así, mientras más pobre es el programa alterno, mayor será el tiempo que el organismo pasará en una alternativa. De esa forma, en la prueba de Belke, la preferencia por el “IV 40 que fue entrenado con el IV 80” respecto al “mismo IV 40 que fue entrenado con el IV 20” se explica porque en el primer programa concurrente (Componente B), el organismo aprendió que debía pasar más tiempo por visita en la opción IV 40, mientras que en el segundo programa (Componente A), este aprendió que debía pasar menos tiempo por visita en la opción IV 40. \n\n\n\n\n## Reflexiones Finales Sobre los Modelos de Elección Basados en Valor\n\n* La igualación de tasas relativas de respuesta a tasas relativas de refuerzo es un fenómeno muy robusto.\n* Igualación es el resultado de un proceso estabilizador de retroalimentación, gobernado por las propiedades temporales de los programas de intervalo.\n* Igualación ilustra la importancia de entender a los programas de refuerzo como restricciones temporales o de respuesta sobre la distribución de comportamientos.\n* En un nivel molecular, la dinámica del movimiento hacia igualación ilustra la relevancia del principio de refuerzo, entendido como un algoritmo de ascenso de colina, como el que vimos en las notas x. Este algoritmo postula que el sistema compara, en cada oportunidad de respuesta, el valor de las variables de decisión asociadas con cada alternativa y elige aquella con el valor más alto. La distribución del comportamiento entre diferentes opciones (en equilibrio) es el resultado de diversos factores: primero, el seguimiento del algoritmo de ascenso de colina por parte del organismo; segundo, los ajustes que sufre dicho algoritmo al operar bajo las diferentes restricciones impuestas por los distintos programas de refuerzo; y tercero, la regla de aprendizaje de los valores de la variable de decisión. \n*La importancia del tiempo de estancia reforzado en cada opción sobre la elección de los organismos. \n*  El avance en los modelos de elección requiere de la consideración de protocolos que capturen la incertidumbre y la volatilidad propia de los entornos naturales de los organismos, un tema que quedará para otra nota.\n","srcMarkdownNoYaml":"\n\n## Arturo Bouzas\n\nEn el capítulo anterior, vimos que estudiando una medida agregada de respuestas y refuerzos, en equilibrio, la tasa relativa de respuestas tiende a igualar a la tasa de refuerzo que produce cada opción. Contemplamos dos posibles explicaciones computacionales para este fenómeno: la igualación de la probabilidad de refuerzo para las opciones de respuesta (igualación de la rentabilidad de las respuestas) y la maximización de la tasa global de refuerzo. \n\nUna alternativa a estos modelos molares son los modelos de *maximización local*, los que asumen que igualación es el resultado de que en cada oportunidad de respuesta, los organismos eligen aquella respuesta asociada con el valor más alto de alguna variable local. Vamos a revisar tres miembros de esta familia de modelos, que se distinguen entre ellos por la variable local que cada uno maximiza:\n\n* *Maximización Momentánea*: Modelos de elección basados en las probabilidades instantáneas de refuerzo de cada alternativa.\n* *Mejoramiento*: Modelos de elección basados en las tasas locales de refuerzo asociadas con cada alternativa.\n* *Valor Q de la respuesta*.\n\n## Maximización Momentánea\n\nEste modelo asume que los organismos computan las probabilidades locales de refuerzo asociadas con cada respuesta. Consideren un programa concurrente IV - IV. En esos programas, la probabilidad de un refuerzo para una respuesta en cada tecla es una función del tiempo transcurrido desde la última visita a una de ellas. Conforme incrementa el tiempo de estancia en una opción, incrementa la probabilidad de un refuerzo para la respuesta en la opción alterna. Este modelo fue propuesto por Shimp en 1969, quien argumentó que el cambio de una opción a otra era controlado por los cambios en la probabilidad de refuerzo asociados con el tiempo transcurrido en cada opción.  Para él, la igualación global era el resultado de una regla de respuesta seguida por el organismo en la cual este respondía a la tecla que tuviera la mayor probabilidad de refuerzo al momento de la elección. Simulando este modelo, Shimp encontró que a nivel global este algoritmo resulta efectivamente en tasas de respuesta iguales a las tasas de refuerzo: reproduciendo así el patrón de igualación.\n\n### Evaluación experimental del modelo de maximización momentánea\n\n#### 1. Programas de refuerzo concurrente IV - IV de ensayos discretos\n\nEmpíricamente, el modelo de maximización instantánea predice regularidades en la estructura de los cambios del organismo de una opción a otra después de ciertas secuencias de respuesta. Si se computan las probabilidades de refuerzo de cada una de las opciones de respuesta, es posible determinar cuál respuesta debe seguir después de una secuencia de *n* respuestas consecutivas en una misma opción. La forma más accesible de estudiar esta predicción es empleando procedimientos de programas de refuerzo concurrentes IV - IV, pero de ensayos discretos. En estos programas, al animal se le presenta la oportunidad de elegir entre dos alternativas con una sola respuesta. Las dos opciones se presentan durante un breve periodo de tiempo. Después de una respuesta a alguna de las dos alternativas, inicia un breve intervalo entre ensayos sin opción de respuesta: al final de este intervalo, al organismo se le presenta una vez más una breve oportunidad para elegir entre una de las dos respuestas. Lo que es importante tener presente frente a estos protocolos es que los programas de refuerzo siguen corriendo durante los intervalos entre ensayos como si no hubiese discontinuidades en el tiempo. Al igual que en los programas IV - IV que hemos revisado, una vez que un refuerzo está disponible para una de las opciones, el reloj del programa IV se detiene y el refuerzo se guarda hasta que es recogido por el animal. Con este protocolo, mientras mayor es el número de elecciones repetidas por una de las opciones, mayor es la probabilidad de que un refuerzo esté esperando en la opción alterna. \n\nEn experimentos con programas de ensayos discretos, se mide la probabilidad de cambiar de alternativa después de varias secuencias de respuesta a una misma tecla. Mientras mayor sea el número de respuestas seguidas a una misma tecla, mayor será la probabilidad de refuerzo asociada con la respuesta alterna. De acuerdo al modelo de maximización instantánea, debería observarse que la probabilidad de cambiar de alternativa es una función del número de respuestas que se hayan dado a la misma tecla. Por ejemplo, si el programa es uno concurrente de tecla verde con IV 1’ - tecla roja con IV 3’, el modelo predice la siguiente secuencia de tres respuestas: Verde-Verde-Roja. Por los valores de los programas de intervalo, después de cada dos respuestas seguidas a la opción verde resulta más probable que se otorgue un refuerzo a la opción roja.\n\nEn el experimento más citado con este procedimiento, Nevin () no encontró evidencia en favor del modelo de maximización instantánea. Ver Figuras. \n\n![](media/17201205364798/17201396263913.jpg)\n\nObsérvese en la figura del panel izquierdo que la probabilidad de refuerzo de la Opción Roja aumenta con la acumulación de elecciones de la Opción Verde por parte del organismo. Al mismo tiempo, en la figura del panel derecho, nótese que la probabilidad de cambiar a la Opción Roja no aumenta con la acumulación de elecciones de la Opción Verde por parte del organismo. Nota: la probabilidad de cambiar de opción se calculó con base al número de oportunidades para cambiar de opción en cada secuencia (run length). Así, estos resultados rompen con el patron de elecciones esperado según el modelo de maximización instantánea. \n\nEn un análisis posterior de su experimento original  y de los datos de un experimento de Silberberg (), Nevin () encontró que la perseverancia en las opciones de respuesta (independientemente de su probabilidad de refuerzo) era el patrón más frecuentemente observado en ambos experimentos. Los animales tendían a quedarse en la tecla a la que habían respondido anteriormente y no a cambiar como una función del número de respuestas a esa tecla Ver fig.\n\n![](media/17201205364798/17201399029035.jpg)\n\n#### 2. Programas de refuerzo concurrente RV - IV de ensayos discretos\n\nUn protocolo adicional para evaluar el modelo de maximización local es un programa concurrente de ensayos discretos RV - IV. En estos programas, la probabilidad de refuerzo para las respuestas asociadas a la opción RV es constante, mientras que la probabilidad de refuerzo para la respuesta asociada al programa IV cambia como una función de la última respuesta a esa opción. Una estrategia consistente con el modelo de maximización instantánea consiste en responder a la opción asociada con el programa RV inmediatamente después de recibir un refuerzo en la opción asociada con el programa IV. Este es el momento con menor probabilidad de refuerzo para la opción IV. De igual forma, la probabilidad de un cambio de la opción RV hacia la opción IV debe incrementar como una función del número de respuestas que se han dado a la opción RV. \n\nLa siguiente figura x presenta los resultados obtenidos por Williams () usando el protocolo anterior. En primer lugar, Williams encontró que los animales igualaban la frecuencia relativa de respuestas a la frecuencia relativa de refuerzos, pero de manera aún más importante para estas notas: no encontró evidencia de que la respuesta fuese controlada por la probabilidad instantánea de refuerzo. El panel izquierdo de la figura muestra la probabilidad de refuerzo como una función del número de ensayos desde la última respuesta a la opción IV. Puede verse que la probabilidad de refuerzo para un cambio a la tecla IV es creciente; consecuentemente, la probabilidad de una respuesta a la tecla IV también debería incrementar como una función del número de ensayos desde la última respuesta a la opción IV. Sin embargo, el panel derecho de la figura muestra que la probabilidad real u observada de una respuesta al IV es constante o decreciente.\n\n![](media/17201205364798/17201421458762.jpg)\n\n\n### Conclusiones acerca del modelo de maximización Instantánea\n\nDe los experimentos con el protocolo de ensayos discretos recién descritos y muchos otros que no presentamos se pueden alcanzar las siguientes conclusiones:\n\n* Consistente con la ley del efecto, la probabilidad de que se repita una opción de respuesta incrementa si esta es seguida por un refuerzo. \n* También se observa un *efecto de perseverancia*: es más probable que se repita la respuesta a una opción que ya fue elegida en el pasado, independientemente de si esta respuesta es reforzada o no. \n* Es importante agregar que la consistencia de los resultados descritos también depende de otras variables como el intervalo entre opciones de elección, el cual afecta la memoria del organismo sobre sus elecciones previas.\n\n## Modelo de Mejoramiento\n\nEl modelo de mejoramiento de Herrnstein y Vaughan () describe la dinámica de elección en situaciones de elección recurrentes, momento a momento. Como otros modelos de elección basados en el valor de las opciones, el modelo de mejoramiento cuenta con dos componentes: \n\n* El primer componente es la especificación de la variable de decisión a partir de la cual el organismo elige (es decir, la variable que guía la elección de los organismos; ejemplos de estas variables son la probabilidad de refuerzo, el tiempo transcurrido en una opción o las tasas locales de refuerzo, entre otras opciones).\n\n* El segundo componente es *la regla de respuesta*: esto es, con qué criterio elige el organismo.\n\nPara el modelo de mejoramiento, la variable de decisión son las tasas locales de refuerzo, a las cuales previamente llamamos como la rentabilidad de las opciones. Esta variable se computa dividiendo el número de refuerzos para cada opción entre el tiempo asignado a cada una de ellas:\n\n$$\\frac {r_i}{T_i}$$\n\nLa rentabilidad puede computarse también para respuestas:\n\n$$\\frac {r_i}{R_i}$$\n\nAsumiendo un tiempo total fijo, de acuerdo al modelo de mejoramiento, cada incremento en una unidad de tiempo asignada a una opción tiene como consecuencia la actualización de las dos tasas de refuerzo locales. Al incrementar el tiempo asignado $t_i$ a una de las opciones, la tasa local de refuerzo de esa opción disminuye (dado que el denominador de la rentabilidad de esa opción crece); simultáneamente, este cambio también reduce el tiempo $t_2$ asignado a la otra opción (dado que el tiempo total es fijo), lo cual incrementa la tasa de refuerzo local asociada con la segunda opción (puesto que el denominador de esta segunda rentabilidad se achica).  \n\nEl segundo componente del modelo de mejoramiento, la regla de elección, es una variante de maximización que consiste en seleccionar la opción con la mejor tasa de refuerzo local dentro de cada oportunidad.\n\nEste modelo describe la elección dentro de un programa concurrente que se construye como un sistema dinámico y de retroalimentación. Es decir, bajo este arreglo, la opción de respuesta que tiene la mayor tasa local de refuerzo va cambiando como una función de la asignación de tiempos y respuestas por parte del organismo. Es por ello que este modelo puede dar cuenta de la dinámica global de acercamiento del organismo al patrón de igualación observado en programas concurrentes IV - IV. Para entender al modelo, tengan presente que al elegir la opción con la mejor tasa de refuerzo local, el organismo está incrementando el tiempo de estancia en esa opción: lo cual aumenta la base temporal *t* para computar la tasa de refuerzo local de esa opción. Simultáneamente, y dado el tiempo total fijo, este comportamiento reduce el tiempo de estancia en la segunda opción, lo cual incrementa la tasa local de refuerzo de esa segunda opción. De esta forma, cada aumento de una unidad de tiempo de estancia en la mejor opción por parte del organismo, tiene dos consecuencias: reduce la tasa de refuerzo local de esa opción e incrementa la tasa de refuerzo local de la otra opción. La transición del organismo de una opción a la otra ocurre cuando la dirección de la diferencia entre las dos tasas cambia de signo y, básicamente, la opción alterna se vuelve mejor que la opción actual. Pueden observar que en estos programas, el modelo de mejoramiento es uno de retroalimentación y corresponde a un algoritmo cuyo blanco es la reducción de la diferencia entre las dos tasas de refuerzo locales; el seguimiento de este algoritmo a la larga (o en equilibrio) resulta en la igualación global de las tasas de respuesta a las tasas de refuerzo para ambas opciones (el patrón de igualación).\nConsideren como ejemplo un programa concurrente IV 1 min - IV 2 min. Supongan que al inicio de la sesión de una hora, el organismo asigna la mitad de su tiempo a cada alternativa.\nSupuesto importante: Vamos a asumir que el animal responde a una tasa moderada que garantiza que en ambos programas de IV el organismo obtendrá el máximo número de reforzadores posibles (para 1 hora), independientemente del tiempo que efectivamente dedique a cada opción durante la sesión. Esto ocurre porque en los programas de intervalo variable, los reforzadores se \"acumulan\" durante el tiempo que el animal no está respondiendo a esa opción. Por lo tanto, aunque el animal dedique menos de 1 hora a una opción, si este responde de manera suficientemente rápida, efectivamente podría obtener todos los reforzadores correspondientes a 1 hora pero en una menor cantidad de tiempo.\nDado este supuesto, las tasas de refuerzo locales para las dos opciones se calculan como el número máximo posible de refuerzos en una hora dividido por la proporción de tiempo realmente asignada a cada opción. Así, la tasa local de refuerzo para la tecla IV 1' sería de 60 reforzadores máximos / 0.5 hr = 120 refuerzos por hora. Una vez más, esto significa que aunque el animal solo dedica media hora a esta opción, este puede obtener los 60 reforzadores disponibles durante la sesión completa gracias a la acumulación propia de los programas IV. Nótese que si el organismo le asigna toda la hora a responder a este programa, su tasa local de refuerzo será de 60/1 hr = 60 refuerzos por hora. Al reducir el tiempo asignado a esa opción a media hora (0.5 hr), la rentabilidad de esa opción aumenta a 120 refuerzos por hora, aunque el número absoluto de reforzadores (60) permanece constante. Para el IV 2', la tasa local de refuerzo es de 30 reforzadores máximos / 0.5 hr = 60 refuerzos por hora.\nDados estos resultados y de acuerdo al modelo de mejoramiento, el animal debería escoger asignar más tiempo a la opción con el programa IV 1' que a la opción con el programa IV 2’, ya que la primera presenta una mayor tasa de refuerzo local (120 vs 60).\nSupongan ahora que como resultado de asignar más tiempo a la opción IV 1', el organismo termina dedicando el 90% de su tiempo a esta opción. Ahora, para la opción asociada con el IV 1 min, la tasa de refuerzo local sería de 60/0.9 hr = 66.7 refuerzos por hora. En el IV 2 min, la tasa de refuerzo local sería de 30/0.1 hr = 300 refuerzos por hora. Recordamos que los programas en sí no han cambiado: lo único que está modificando la rentabilidad percibida de ambas opciones es la manera en la que el organismo distribuye su tiempo entre ellas. \nPueden observar que al incrementar el tiempo asignado al IV 1', paradójicamente se incrementó la tasa local de refuerzo asociada con la opción IV 2' (de 60 a 300 refuerzos por hora) y por lo tanto, si el organismo sigue la regla de mejoramiento, su subsecuente elección debe ser la opción del IV 2'.\nBajo este modelo, el equilibrio se alcanza cuando la proporción de tiempo asignado a las dos opciones por parte del organismo es de 2/3 para IV 1' (60/0.666  90) y 1/3 para IV 2' (30/0.333  90). Lo anterior es aproximadamente igual a \"90 refuerzos por hora\" en ambas alternativas. Así, en el punto de equilibrio, ambas alternativas ofrecen exactamente la misma tasa de refuerzo local (90 por hora), lo que explica por qué el organismo deja de cambiar su distribución de tiempo entre las opciones tras alcanzar este punto. Este resultado es equivalente a la **igualación** global de tasas relativas de respuesta a las tasas relativas de refuerzo: \n0.66 hr/(0.66 hr + 0.33 hr)  60 r/(60 r + 30 r)\nEn el correspondiente simulador, ustedes podrán ver la dinámica del sistema para diferentes valores de programas concurrentes.\n\nUn problema importante del modelo de mejoramiento es que deja sin especificar la ventana temporal que los organismos requieren para computar las tasas de refuerzo locales de las distintas opciones. ¿Estas tasas se estiman hasta finalizar la duración de cada sesión experimental? O bien aún, ¿seguirá el organismo algoritmos menos evidentes? Por ejemplo, ¿reiniciar la computación de las tasas para las dos opciones cada 10 minutos?¿O borrar la historia con las opciones de respuesta experimentadas días atrás? Las respuestas a todas estas preguntas no son evidentes. Y al mismo tiempo, estas tienen importantes implicaciones para la aplicación del modelo a entornos volátiles, los cuales frecuentemente cambian las condiciones de refuerzo para las diferentes opciones a lo largo del tiempo. Este tema será abordado en otras notas.\n\n## Modelo de Valor *Q* de la respuesta\n\nDe los modelos que dan cuenta de la elección recurrente, el modelo de valor *Q* que vimos en las notas x es el más cercano a la ley del efecto original planteada por Thorndike. La variable de decisión de este modelo es el valor *Q* adquirido por cada opción de respuesta. Este valor representa la integración de la historia de reforzamiento de cada opción que resulta de la regla del error de predicción. Por otra parte, la regla de respuesta bajo este modelo consiste en la elección probabilística de la respuesta con mayor valor *Q* en cada oportunidad. Presten atención a que en este modelo, cada respuesta adquiere su valor exclusivamente como función de los refuerzos que produce y es independiente del valor de las demás alternativas presentes.\n\nPor consecuente, una predicción importante de este modelo es que cuando al organismo se le presenta la oportunidad de elegir entre dos respuestas con diferentes valores *Q*, su elección debe ser independiente del contexto donde fue adquirido el valor *Q* de cada opción. Imaginen que en su ciudad hay tres cadenas de cafeterías (A, B y C). Cerca de su casa y cerca de su trabajo, hay dos sucursales disponibles (digamos que A y B están cerca de su casa; y B y C están cerca de su trabajo). Una de las cadenas de cafetería, la B, tiene una sucursal en los dos escenarios donde ustedes compran café (a la sucursal cerca de su casa le llamaremos B’ y a la sucursal cerca de su trabajo le llamaremos B’’). Las sucursales de la cadena B tienen un logo que las distingue. Digamos que sus menús son similares, por lo que las dos sucursales B, aunque tienen sutiles diferencias en cuanto a su staff, comparten a grandes rasgos el mismo programa de recompensas (B’ = B’’). En cambio, las cafeterías A y C son claramente distinguibles en cuanto a calidad con relación a ambas cafeterías B. En resumen: A es mucho mejor que B (A > B) y C es mucho peor que B (C < B). Sin embargo, un domingo ustedes acuden a otra zona de la ciudad y se dan cuenta de que las dos sucursales de las cafeterías B (B’ y B’’) han sido reubicadas ahora en una misma y nueva zona de la ciudad. Dado que los programa de recompensas de ambas opciones son iguales (B’ = B’’), ustedes deberían ser indiferentes entre ellas. Sin embargo, es posible que los valores percibidos de B’ y de B’’ dependan de cuál era el otro restaurante con el que ambas opciones competían dentro de su contexto previo (A o C).  \nUna forma de evaluar experimentalmente esta predicción sobre la relevancia del contexto previo de los refuerzos es presentar a los animales con dos situaciones de elección diferentes:\nPrograma concurrente 1: El animal puede elegir entre:\nOpción A: reforzada con intervalo variable IV(a)\nOpción B: reforzada con intervalo variable IV(b)\nPrograma concurrente 2: El animal puede elegir entre:\nOpción A: reforzada con el mismo intervalo variable IV(a) que en la Situación 1\nOpción B': reforzada con un intervalo variable IV(b') diferente al de la Situación 1\nPosteriormente, se evalúa la preferencia del animal entre las opciones A de ambos programas concurrentes, las cuales comparten exactamente el mismo valor de reforzamiento (Q). Bajo este protocolo, cada par de opciones (cada componente o programa concurrente) se encuentra vigente en períodos de tiempo separados dentro de una misma sesión (formalmente a este arreglo se le conoce como programas múltiples de refuerzo). Una vez alcanzado el equilibrio en cada programa concurrente IV - IV: se presenta una nueva combinación de las opciones comunes a ambos programas. De acuerdo a los modelos de refuerzo, de mejoramiento y de valor Q, la elección en los periodos de prueba debe reflejar los valores adquiridos por cada opción individual.\n\nBelke () corrió una versión de este protocolo. Para uno de los componentes de programas concurrentes (Componente A), los valores de las opciones eran Tecla Blanca (IV 20) vs Tecla Roja (IV 40), mientras que para el otro programa concurrente (el Componente B), las opciones eran Tecla Verde con IV 40 vs una Tecla Amarilla con IV 80. Así, para el Componente A, la opción del IV 40 ocurría en el contexto de una opción IV 20 (la Tecla Roja) que era dos veces más rica. Mientras tanto, para el segundo par de estímulos (el Componente B), la opción IV 40 (la Tecla Verde) se presentaba en el contexto de otra opción reforzada con un IV 80 (la Tecla Amarilla), la cual era dos veces menos rica. De acuerdo al modelo *Q* que no considera el contexto de los reforzadores, las dos opciones de respuesta reforzadas con el IV 40 (las Teclas Roja y Verde) en ambos programas deben tener el mismo valor. Sin embargo, otra posibilidad es que el valor adquirido por una opción de respuesta dependa del contexto de los refuerzos proporcionados a las respuestas alternativas presentes. En este caso, la Tecla Verde reforzada con el IV 40 dentro del contexto de otra opción reforzada con un IV pobre, adquiere un valor más grande que la Tecla Roja, también reforzada con un IV 40 pero que fue entrenada previamente en un contexto con una opción reforzada con un IV más rico. \n\nContrario a la predicción de indiferencia arrojada por el modelo de refuerzo *Q*, Belke encontró que los animales sí preferían la opción IV 40 entrenada en el contexto pobre, por encima de la misma opción entrenada en el contexto rico. Esta evidencia sugiere que el impacto de los reforzadores sobre el valor de una respuesta depende de los reforzadores obtenidos por las otras opciones presentes en el pasado (es decir, con qué otros estímulos ha convivido cada estímulo previamente). \n\nLos resultados anteriores son consistentes con la siguiente interpretación más local de la ejecución en programas concurrentes. La interpretación consiste en suponer que en estos programas lo que el animal aprende es el tiempo que debe pasar en una opción antes de cambiar a otra alternativa. Suponemos que estos tiempos se encuentran relacionados linealmente a la tasa de refuerzo en la tecla alternativa: así, mientras más pobre es el programa alterno, mayor será el tiempo que el organismo pasará en una alternativa. De esa forma, en la prueba de Belke, la preferencia por el “IV 40 que fue entrenado con el IV 80” respecto al “mismo IV 40 que fue entrenado con el IV 20” se explica porque en el primer programa concurrente (Componente B), el organismo aprendió que debía pasar más tiempo por visita en la opción IV 40, mientras que en el segundo programa (Componente A), este aprendió que debía pasar menos tiempo por visita en la opción IV 40. \n\n\n\n\n## Reflexiones Finales Sobre los Modelos de Elección Basados en Valor\n\n* La igualación de tasas relativas de respuesta a tasas relativas de refuerzo es un fenómeno muy robusto.\n* Igualación es el resultado de un proceso estabilizador de retroalimentación, gobernado por las propiedades temporales de los programas de intervalo.\n* Igualación ilustra la importancia de entender a los programas de refuerzo como restricciones temporales o de respuesta sobre la distribución de comportamientos.\n* En un nivel molecular, la dinámica del movimiento hacia igualación ilustra la relevancia del principio de refuerzo, entendido como un algoritmo de ascenso de colina, como el que vimos en las notas x. Este algoritmo postula que el sistema compara, en cada oportunidad de respuesta, el valor de las variables de decisión asociadas con cada alternativa y elige aquella con el valor más alto. La distribución del comportamiento entre diferentes opciones (en equilibrio) es el resultado de diversos factores: primero, el seguimiento del algoritmo de ascenso de colina por parte del organismo; segundo, los ajustes que sufre dicho algoritmo al operar bajo las diferentes restricciones impuestas por los distintos programas de refuerzo; y tercero, la regla de aprendizaje de los valores de la variable de decisión. \n*La importancia del tiempo de estancia reforzado en cada opción sobre la elección de los organismos. \n*  El avance en los modelos de elección requiere de la consideración de protocolos que capturen la incertidumbre y la volatilidad propia de los entornos naturales de los organismos, un tema que quedará para otra nota.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Capítulo 10.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.36","title":"Comportamiento de Elección: Maximización Local "},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}